{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ca0fb-e358-4dbe-9951-e5167758d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gzip\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "import numpy as np\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm.notebook\n",
    "\n",
    "client = Client(\"tls://localhost:8786\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55ec891-e0c2-4f43-ba35-2077ecafcc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(max_size_GB = None):\n",
    "    with gzip.open(\"file_metadata_v2.json.gz\") as f:\n",
    "        dataset_info = json.loads(f.read().decode())\n",
    "\n",
    "    all_files = []\n",
    "    all_sizes_GB = []\n",
    "    for containers_for_category in dataset_info.values():\n",
    "        for container, metadata in containers_for_category.items():\n",
    "            if metadata[\"files_output\"] is None:\n",
    "                continue\n",
    "            for fname, size in zip(metadata[\"files_output\"], metadata[\"sizes_output_GB\"]):\n",
    "                all_files.append(fname)\n",
    "                all_sizes_GB.append(size)\n",
    "                if max_size_GB and sum(all_sizes_GB) > max_size_GB:\n",
    "                    return all_files, all_sizes_GB\n",
    "    return all_files, all_sizes_GB\n",
    "\n",
    "all_files, all_sizes_GB = get_input(max_size_GB = None)  # limit list to specific total size\n",
    "print(f\"list of {len(all_files)} files with total size {sum(all_sizes_GB):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7615f661-03bb-488e-955b-dd53771abde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if no filesize metadata exist, parse it from xrdcp\n",
    "# ##################################################\n",
    "\n",
    "# import pexpect\n",
    "\n",
    "# def run_xrdcp(fname):\n",
    "#     t0 = time.time()\n",
    "#     child = pexpect.spawn(f\"xrdcp {fname} /dev/null -f\")\n",
    "#     child.expect(pexpect.EOF, timeout=600)\n",
    "#     t1 = time.time()\n",
    "#     res = child.before.decode()\n",
    "#     size = res.split(\"\\r\")[-2].split(\"/\")[0][1:]\n",
    "#     if \"MB\" in size:\n",
    "#         size_in_GB = float(size[:-2]) * 1024**2 / 1000**3\n",
    "#     elif \"GB\" in size:\n",
    "#         size_in_GB = float(size[:-2]) * (1024/1000)**3\n",
    "#     elif \"kB\" in size:\n",
    "#         size_in_GB = float(size[:-2]) * 1024 / 1000**3\n",
    "#     else:\n",
    "#         raise ValueError(f\"cannot handle size: {size}\")\n",
    "#     return {\"t0\": t0, \"t1\": t1, \"GBread\": size_in_GB}\n",
    "\n",
    "# t0 = time.time()\n",
    "# tasks = [dask.delayed(run_xrdcp)(fname) for fname in all_files]\n",
    "# futures = client.compute(tasks)\n",
    "\n",
    "# with tqdm.notebook.tqdm(total=len(futures)) as pbar:\n",
    "#   for future in dask.distributed.as_completed(futures):\n",
    "#     pbar.update(1)\n",
    "\n",
    "# res = [f.result() for f in futures]\n",
    "# t1 = time.time()\n",
    "# all_sizes_GB_from_xrdcp = [r[\"GBread\"] for r in res]\n",
    "\n",
    "\n",
    "\n",
    "# if filesize metadata is available, this is simpler\n",
    "# ##################################################\n",
    "\n",
    "def run_xrdcp(fname, size):\n",
    "    t0 = time.time()\n",
    "    os.system(f\"xrdcp {fname} /dev/null -f\")\n",
    "    t1 = time.time()\n",
    "    return {\"t0\": t0, \"t1\": t1, \"GBread\": size}\n",
    "\n",
    "t0 = time.time()\n",
    "tasks = [dask.delayed(run_xrdcp)(fname, size) for fname, size in zip(all_files, all_sizes_GB)]\n",
    "futures = client.compute(tasks)\n",
    "\n",
    "with tqdm.notebook.tqdm(total=len(futures)) as pbar:\n",
    "  for future in dask.distributed.as_completed(futures):\n",
    "    pbar.update(1)\n",
    "\n",
    "res = [f.result() for f in futures]\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04faacc-8136-4ef9-910a-98b8660cb4d6",
   "metadata": {},
   "source": [
    "track egress: [link](https://grafana.mwt2.org/d/EKefjM-Sz/af-network-200gbps-challenge?orgId=1&from=now-1h&to=now&viewPanel=panel-205&refresh=5s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd03f327-857a-4d80-a013-3a13549de37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_runtime_sum = sum(r[\"t1\"] - r[\"t0\"] for r in res)\n",
    "\n",
    "print(f\"processtime: {total_runtime_sum:.2f} s\")\n",
    "print(f\" -> data rate per worker: {sum(all_sizes_GB) * 8 / total_runtime_sum:.2f} Gbps\")\n",
    "\n",
    "print(f\"walltime: {t1-t0:.2f} s\")\n",
    "print(f\" -> total data rate: {sum(all_sizes_GB) * 8 / (t1-t0):.2f} Gbps\")\n",
    "\n",
    "starts = np.asarray([r[\"t0\"] for r in res])\n",
    "ends = np.asarray([r[\"t1\"] for r in res])\n",
    "GBread = [r[\"GBread\"] for r in res]\n",
    "rates_per_chunk = GBread / (ends - starts)\n",
    "\n",
    "t_samples = np.linspace(t0, t1, 100)\n",
    "rate_samples = []\n",
    "for t in t_samples:\n",
    "    mask = np.logical_and((starts <= t), (t < ends))\n",
    "    rate_samples.append(float(sum(rates_per_chunk[mask]) * 8))\n",
    "\n",
    "print(f\"total data read from data rate integral: {sum((t_samples[1] - t_samples[0]) * np.asarray(rate_samples)) / 8:.2f} GB\")\n",
    "t_samples = [datetime.datetime.fromtimestamp(t) for t in t_samples.tolist()]\n",
    "\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "ax.plot(t_samples, rate_samples, marker=\"v\", linewidth=0)\n",
    "ax.set_xlabel(\"time\")\n",
    "ax.tick_params(axis=\"x\", labelrotation=45)\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
    "ax.set_ylabel(\"data rate [Gbps]\")\n",
    "ax.set_ylim([0, ax.get_ylim()[1] * 1.1])\n",
    "fig.savefig(\"xrdcp_rate.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
