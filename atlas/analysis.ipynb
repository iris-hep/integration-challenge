{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "021c38db-e1c9-4137-b7f4-87a094ca5108",
   "metadata": {},
   "source": [
    "# ATLAS Integration Challenge\n",
    "\n",
    "The next cell updates dependencies, make sure to restart the kernel afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f15ac-903b-4f33-b775-e7c14a3647a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade --quiet atlas_schema\n",
    "! pip install --upgrade --quiet --pre mplhep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b8b953-dc21-4d5c-899d-b1f0b03c70b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gzip\n",
    "import json\n",
    "import math\n",
    "import pathlib\n",
    "import time\n",
    "\n",
    "import awkward as ak\n",
    "import cloudpickle\n",
    "import dask\n",
    "import dask.bag\n",
    "import hist\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep\n",
    "import numpy as np\n",
    "import uproot\n",
    "import vector\n",
    "\n",
    "from atlas_schema.schema import NtupleSchema\n",
    "from coffea import processor\n",
    "from coffea.nanoevents import NanoEventsFactory\n",
    "from dask.distributed import Client, PipInstall, performance_report\n",
    "from IPython.display import display\n",
    "\n",
    "import utils\n",
    "\n",
    "vector.register_awkward()\n",
    "mplhep.style.use(mplhep.style.ATLAS1)\n",
    "\n",
    "client = Client(\"tls://localhost:8786\")\n",
    "\n",
    "plugin = PipInstall(packages=[\"atlas_schema\"], pip_options=[\"--upgrade\"])\n",
    "client.register_plugin(plugin)\n",
    "\n",
    "cloudpickle.register_pickle_by_value(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bbd464-1423-4353-81cc-f43806f04a7e",
   "metadata": {},
   "source": [
    "### Fileset preparation\n",
    "\n",
    "Comment out the desired version in the next cell to run over the full input dataset or a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536e4026-6004-46ef-9f96-6983ed726957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full scale\n",
    "# fileset, input_size_GB = utils.get_fileset(campaign_filter=None, dsid_filter=None, max_files_per_sample=None)\n",
    "\n",
    "# 9% of full scale, using 2022 data + MC\n",
    "fileset, input_size_GB = utils.get_fileset(campaign_filter=[\"mc23a\", \"data22\"], dsid_filter=None, max_files_per_sample=None)\n",
    "\n",
    "# minimal setup for debugging\n",
    "# fileset, input_size_GB = utils.get_fileset(campaign_filter=[\"mc23a\"], dsid_filter=[\"601229\"], max_files_per_sample=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a081b9-c4ec-41c8-830c-a727e56ff472",
   "metadata": {},
   "source": [
    "### Simple non-distributed reading\n",
    "\n",
    "This cell is for debugging and to ensure the basic non-distributed infrastructure works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6c685f-7e9c-4c5b-8f80-19f1543de32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCPTSchema(NtupleSchema):\n",
    "    # mcChannelNumber not defined for data\n",
    "    event_ids = {\"actualInteractionsPerCrossing\", \"averageInteractionsPerCrossing\", \"eventNumber\", \"mcChannelNumber\", \"runNumber\"}\n",
    "    mixins = {\n",
    "        \"globalTriggerEffSF\": \"Weight\",  # events[\"NOSYS\"].globalTriggerEffSF.emu\n",
    "        \"globalTriggerMatch\": \"Systematic\",  # events[\"NOSYS\"].globalTriggerMatch.emu\n",
    "        **NtupleSchema.mixins\n",
    "    }\n",
    "\n",
    "events = NanoEventsFactory.from_root(\n",
    "    {list(fileset[list(fileset.keys())[0]][\"files\"])[0]: \"reco\"},\n",
    "    mode=\"virtual\",\n",
    "    schemaclass=TCPTSchema,\n",
    "    entry_stop=1000,\n",
    "    access_log=(access_log := []),\n",
    ").events()\n",
    "\n",
    "h = hist.new.Regular(30, 0, 300, label=\"leading electron $p_T$\").StrCat([], name=\"variation\", growth=True).Weight()\n",
    "\n",
    "for variation in events.systematic_names:\n",
    "    if variation != \"NOSYS\" and \"EG_SCALE_ALL\" not in variation:\n",
    "        continue\n",
    "\n",
    "    cut = events[variation][\"pass\"][\"ejets\"] == 1\n",
    "    h.fill(events[variation][cut==1].el.pt[:, 0] / 1_000, variation=variation)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for variation in h.axes[1]:\n",
    "    h[:, variation].plot(histtype=\"step\", label=variation, ax=ax)\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31f4dd8-07aa-4dd0-b50f-013349abe59a",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "This step touches every file to extract metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abaeac0-ca4c-4a36-8426-10438c4e034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = processor.Runner(\n",
    "    executor = processor.DaskExecutor(client=client, treereduction=4),\n",
    "    # executor = processor.IterativeExecutor(),  # to run locally\n",
    "    schema=TCPTSchema,\n",
    "    savemetrics=True,\n",
    "    chunksize=100_000,\n",
    "    skipbadfiles=True,\n",
    "    align_clusters=False,\n",
    "    # maxchunks=1  # for debugging only\n",
    ")\n",
    "\n",
    "\n",
    "def extract_sumw(f):\n",
    "    \"\"\"read initial sum of weights, custom function to be injected during pre-processing\"\"\"\n",
    "    matching_histograms = f.keys(filter_name=\"CutBookkeeper*NOSYS\")\n",
    "    if len(matching_histograms):\n",
    "        sumw = float(f[matching_histograms[0]].values()[1])\n",
    "    else:\n",
    "        sumw = 0  # for data\n",
    "    return {\"sumw\": sumw}\n",
    "\n",
    "\n",
    "with performance_report(filename=\"preprocess.html\"):\n",
    "    # custom pre-processing: like coffea, but with more metadata\n",
    "    t0 = time.time()\n",
    "    preprocess_output = utils.custom_preprocess(fileset, client=client, chunksize=run.chunksize, custom_func=extract_sumw)\n",
    "    t1 = time.time()\n",
    "\n",
    "\n",
    "# calculate dataset-aggregated sumw (without chunk duplication) and update preprocessing metadata accordingly\n",
    "sumw_dict = collections.defaultdict(dict)\n",
    "for chunk in preprocess_output:\n",
    "    sumw_dict[chunk.dataset][chunk.filename] = chunk.usermeta[\"sumw\"]\n",
    "\n",
    "for chunk in preprocess_output:\n",
    "    # for data all sumw entries are 0, set the total to 1 manually\n",
    "    chunk.usermeta.update({\"sumw_dataset\": sum(sumw_dict[chunk.dataset].values()) or 1})\n",
    "\n",
    "\n",
    "# visualize task stream\n",
    "ts = client.get_task_stream(start=f\"{math.ceil(time.time()-t0)}s\")\n",
    "_ = utils.plot_taskstream(ts)\n",
    "\n",
    "print(f\"generated list of {len(preprocess_output)} work items in {t1-t0:.1f} sec:\\n{preprocess_output[:3]}\")\n",
    "\n",
    "# write to disk\n",
    "with open(\"preprocess_output.json\", \"w\") as f:\n",
    "    json.dump(utils.preprocess_to_json(preprocess_output), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4667b1bf-0ff3-4ccf-93e9-4f4a8e0aa3c7",
   "metadata": {},
   "source": [
    "### Processing\n",
    "\n",
    "This either uses `coffea`'s `Runner` interface or a custom `dask.bag` method for more metadata tracking.\n",
    "Configure this via `USE_CUSTOM_PROCESSING`.\n",
    "The custom approach is needed for instantaneous data rate calculation in this notebook.\n",
    "\n",
    "Track XCache egress: [link](https://grafana.mwt2.org/d/EKefjM-Sz/af-network-200gbps-challenge?orgId=1&from=now-1h&to=now&viewPanel=panel-205&refresh=5s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a325d073-d017-4443-a539-b64d9b8feeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure whether to use coffea Runner interface or custom dask.bag-based version\n",
    "USE_CUSTOM_PROCESSING = True\n",
    "\n",
    "\n",
    "class Analysis(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        self.h = hist.new.Regular(20, 0, 1_000, label=\"$m_{jj}$ [GeV]\").\\\n",
    "            StrCat([], name=\"category\", growth=True).\\\n",
    "            StrCat([], name=\"variation\", growth=True).\\\n",
    "            Weight()\n",
    "\n",
    "    def process(self, events):\n",
    "        sumw = events.metadata[\"sumw_dataset\"]\n",
    "        for variation in events.systematic_names:\n",
    "            if variation not in [\"NOSYS\"] + [name for name in events.systematic_names if \"JET_JER_Effective\" in name]:\n",
    "                continue\n",
    "\n",
    "            cut = events[variation][\"pass\"][\"ejets\"] == 1\n",
    "            # TODO: remaining weights\n",
    "            weight = (events[variation][cut==1].weight.mc if events.metadata[\"dsid\"] != \"data\" else 1.0) * events.metadata[\"weight_xs\"] * events.metadata[\"lumi\"] / sumw\n",
    "            mjj = (events[variation][cut==1].jet[:, 0] + events[variation][cut==1].jet[:, 1]).mass\n",
    "            self.h.fill(mjj / 1_000, category=events.metadata[\"category\"], variation=variation, weight=weight)\n",
    "\n",
    "        return {\"hist\": self.h, \"meta\": {}}\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        pass\n",
    "\n",
    "\n",
    "# load pre-processing information from disk\n",
    "with open(\"preprocess_output.json\") as f:\n",
    "    preprocess_output = utils.json_to_preprocess(json.load(f))\n",
    "\n",
    "client.run_on_scheduler(utils.start_tracking)  # track worker count on scheduler\n",
    "t0 = time.time()  # track walltime\n",
    "\n",
    "if USE_CUSTOM_PROCESSING:\n",
    "    # configure here whether to preload branches\n",
    "    columns_to_preload = json.load(pathlib.Path(\"columns_to_preload.json\").open())[\"JET_JER_Effective\"]\n",
    "    columns_to_preload = []\n",
    "\n",
    "    with performance_report(filename=\"process_custom.html\"):\n",
    "        out, report = utils.custom_process(preprocess_output, processor_class=Analysis, schema=run.schema, client=client, preload=columns_to_preload)\n",
    "    print(f\"preloaded columns: {len(columns_to_preload)}, {columns_to_preload} {\"etc.\" if len(columns_to_preload) > 4 else \"\"}\")\n",
    "    print(f\"preloaded but unused columns: {len([c for c in columns_to_preload if c not in report[\"columns\"]])}\")\n",
    "    print(f\"used but not preloaded columns: {len([c for c in report[\"columns\"] if c not in columns_to_preload])}\")\n",
    "\n",
    "    # shortened version of report, dropping extra columns and per-chunk information\n",
    "    display(\n",
    "        dict((k, v) for k, v in report.items() if k not in [\"columns\", \"chunk_info\"]) | \n",
    "        {\"columns\": report[\"columns\"][0:10] + [\"...\"]} | \n",
    "        {\"chunk_info\": list(report[\"chunk_info\"].items())[:2] + [\"...\"]}\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # coffea Runner-based processing\n",
    "    with performance_report(filename=\"process.html\"):\n",
    "        out, report = run(preprocess_output, processor_instance=Analysis())\n",
    "\n",
    "    # shortened version of report, dropping extra columns\n",
    "    display(dict((k, v) for k, v in report.items() if k != \"columns\") | ({\"columns\": report[\"columns\"][0:10] + [\"...\"]}))\n",
    "\n",
    "t1 = time.time()\n",
    "worker_count_dict = client.run_on_scheduler(utils.stop_tracking)  # stop tracking, read out data, get average\n",
    "nworker_avg = utils.get_avg_num_workers(worker_count_dict)\n",
    "\n",
    "print(f\"histogram size: {out[\"hist\"].view(True).nbytes / 1_000 / 1_000:.2f} MB\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9575019b-d1a5-4a8e-9d5a-32d0d8bd0919",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"walltime: {t1 - t0:.2f} sec ({(t1 - t0) / 60:.2f} min)\")\n",
    "print(f\"average worker count: {nworker_avg:.1f}\")\n",
    "print(f\"number of events processed: {report[\"entries\"]:,}\\n\")\n",
    "\n",
    "print(f\"data read: {report[\"bytesread\"] / 1000**3:.2f} GB in {report[\"chunks\"]} chunks (average {report[\"bytesread\"] / 1000**3 / report[\"chunks\"]:.2f} GB per chunk)\")\n",
    "print(f\"average total data rate: {report[\"bytesread\"] / 1000**3 * 8 / (t1 - t0):.2f} Gbps\")\n",
    "print(f\"fraction of input files read: {report[\"bytesread\"] / 1000**3 / input_size_GB:.1%}\")\n",
    "print(f\"number of branches read: {len(report[\"columns\"])}\\n\")\n",
    "\n",
    "print(f\"worker-average event rate using \\'processtime\\': {report[\"entries\"] / 1000 / report[\"processtime\"]:.2f} kHz\")\n",
    "print(f\"worker-average data rate using \\'processtime\\': {report[\"bytesread\"] / 1000**3 * 8 / report[\"processtime\"]:.2f} Gbps\\n\")\n",
    "\n",
    "print(f\"average event rate using walltime and time-averaged worker count: {report[\"entries\"] / 1000 / (t1 - t0) / nworker_avg:.2f} kHz\")\n",
    "print(f\"average data rate using walltime and time-averaged worker count: {report[\"bytesread\"] / 1000**3 * 8 / (t1 - t0) / nworker_avg:.2f} Gbps\\n\")\n",
    "\n",
    "print(f\"fraction of time spent in processing: {report[\"processtime\"] / ((t1 - t0) * nworker_avg):.1%}\")\n",
    "print(f\"average process task length: {report[\"processtime\"] / report[\"chunks\"]:.1f} sec\")\n",
    "\n",
    "timestamps, datarates = utils.calculate_instantaneous_rates(t0, t1, report, num_samples=50)\n",
    "_ = utils.plot_worker_count(worker_count_dict, timestamps, datarates)\n",
    "\n",
    "# visualize task stream\n",
    "ts = client.get_task_stream(start=f\"{math.ceil(time.time()-t0)}s\")\n",
    "_ = utils.plot_taskstream(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982cce52-7f5c-4126-a5cc-6a4dfee70732",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_stack = []\n",
    "labels = []\n",
    "for cat in sorted(out[\"hist\"].axes[1]):\n",
    "    if cat in [\"data\", \"ttbar_H7\", \"ttbar_hdamp\", \"ttbar_pthard\", \"Wt_DS\", \"Wt_H7\", \"Wt_pthard\"]:\n",
    "        continue  # data drawn separately, skip MC modeling variations\n",
    "\n",
    "    mc_stack.append(out[\"hist\"][:, cat, \"NOSYS\"])\n",
    "    labels.append(cat)\n",
    "\n",
    "try:\n",
    "    data_hist = out[\"hist\"][:, \"data\", \"NOSYS\"]\n",
    "except KeyError:\n",
    "    print(\"falling back to plotting first entry of categorical axes as \\\"data\\\"\")\n",
    "    data_hist = out[\"hist\"][:, 0, 0]\n",
    "\n",
    "fig, ax1, ax2 = mplhep.comp.data_model(\n",
    "    data_hist=data_hist,\n",
    "    stacked_components=mc_stack,\n",
    "    stacked_labels=labels,\n",
    "    # https://scikit-hep.org/mplhep/gallery/model_with_stacked_and_unstacked_histograms_components/\n",
    "    # unstacked_components=[],\n",
    "    # unstacked_labels=[],\n",
    "    xlabel=out[\"hist\"].axes[0].label,\n",
    "    ylabel=\"Entries\",\n",
    ")\n",
    "\n",
    "mplhep.atlas.label(\"Internal\", ax=ax1, data=True, lumi=f\"{utils.integrated_luminosity(\"\", total=True) / 1000:.0f}\", com=\"13/ \\\\ 13.6 \\\\ TeV\")\n",
    "mplhep.mpl_magic(ax=ax1)\n",
    "ax2.set_ylim([0.5, 1.5])\n",
    "\n",
    "# compare to e.g. https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/HDBS-2020-11/fig_02a.png\n",
    "fig.savefig(\"mjj.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e3efe0-f724-4206-b233-202a51729014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "import uhi.io.json\n",
    "\n",
    "with gzip.open(\"hist.json.gz\", \"w\") as f:\n",
    "    f.write(json.dumps(out[\"hist\"], default=uhi.io.json.default).encode(\"utf-8\"))\n",
    "\n",
    "with gzip.open(\"hist.json.gz\") as f:\n",
    "    h = hist.Hist(json.loads(f.read(), object_hook=uhi.io.json.object_hook))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
