{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f15ac-903b-4f33-b775-e7c14a3647a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade atlas_schema\n",
    "# ! pip install --upgrade git+https://github.com/scikit-hep/mplhep.git\n",
    "\n",
    "# import importlib\n",
    "# importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b8b953-dc21-4d5c-899d-b1f0b03c70b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import re\n",
    "\n",
    "import awkward as ak\n",
    "import dask\n",
    "import vector\n",
    "import hist\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep\n",
    "import numpy as np\n",
    "import uproot\n",
    "\n",
    "from atlas_schema.schema import NtupleSchema\n",
    "from coffea import processor\n",
    "from coffea.nanoevents import NanoEventsFactory\n",
    "from dask.distributed import Client, PipInstall\n",
    "\n",
    "\n",
    "import utils\n",
    "\n",
    "vector.register_awkward()\n",
    "mplhep.style.use(mplhep.style.ATLAS1)\n",
    "\n",
    "client = Client(\"tls://localhost:8786\")\n",
    "\n",
    "plugin = PipInstall(packages=[\"atlas_schema\"], pip_options=[\"--upgrade\"])\n",
    "client.register_plugin(plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bbd464-1423-4353-81cc-f43806f04a7e",
   "metadata": {},
   "source": [
    "### fileset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcf6216-0eca-4ea0-921b-eae3eda04af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata from file\n",
    "fname = \"ntuple_production/file_metadata.json.gz\"\n",
    "with gzip.open(fname) as f:\n",
    "    dataset_info = json.loads(f.read().decode())\n",
    "\n",
    "# construct fileset\n",
    "fileset = {}\n",
    "for containers_for_category in dataset_info.values():\n",
    "    for container, metadata in containers_for_category.items():\n",
    "        if metadata[\"files_output\"] is None:\n",
    "            # print(f\"skipping missing {container}\")\n",
    "            continue\n",
    "\n",
    "        dsid, _, campaign = utils.dsid_rtag_campaign(container)\n",
    "        weight_xs = utils.sample_xs(campaign, dsid)\n",
    "        lumi = utils.integrated_luminosity(campaign)\n",
    "        # TODO remove limit of nfiles per container\n",
    "        fileset[container] = {\"files\": dict((path, \"reco\") for path in metadata[\"files_output\"][:5]), \"metadata\": {\"dsid\": dsid, \"campaign\": campaign, \"weight_xs\": weight_xs, \"lumi\": lumi}}\n",
    "\n",
    "# minimal fileset for debugging\n",
    "# fileset = {\"mc20_13TeV.601352.PhPy8EG_tW_dyn_DR_incl_antitop.deriv.DAOD_PHYSLITE.e8547_s4231_r13144_p6697\": fileset[\"mc20_13TeV.601352.PhPy8EG_tW_dyn_DR_incl_antitop.deriv.DAOD_PHYSLITE.e8547_s4231_r13144_p6697\"]}\n",
    "# fileset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a081b9-c4ec-41c8-830c-a727e56ff472",
   "metadata": {},
   "source": [
    "### simple non-distributed reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6c685f-7e9c-4c5b-8f80-19f1543de32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = NanoEventsFactory.from_root(\n",
    "    {list(fileset[list(fileset.keys())[0]][\"files\"])[0]: \"reco\"},\n",
    "    mode=\"virtual\",\n",
    "    schemaclass=NtupleSchema,\n",
    "    entry_stop=1000\n",
    ").events()\n",
    "\n",
    "h = hist.new.Regular(30, 0, 300, label=\"leading electron $p_T$\").StrCat([], name=\"variation\", growth=True).Weight()\n",
    "\n",
    "for variation in events.systematic_names:\n",
    "    if variation != \"NOSYS\" and \"EG_SCALE_ALL\" not in variation:\n",
    "        continue\n",
    "\n",
    "    cut = events[variation][\"pass\"][\"ejets\"] == 1\n",
    "    h.fill(events[variation][cut==1].el.pt[:, 0] / 1_000, variation=variation)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for variation in h.axes[1]:\n",
    "    h[:, variation].plot(histtype=\"step\", label=variation, ax=ax)\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31f4dd8-07aa-4dd0-b50f-013349abe59a",
   "metadata": {},
   "source": [
    "### pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abaeac0-ca4c-4a36-8426-10438c4e034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = processor.Runner(\n",
    "    executor = processor.DaskExecutor(client=client),\n",
    "    # executor = processor.IterativeExecutor(),\n",
    "    schema=NtupleSchema,\n",
    "    savemetrics=True,\n",
    "    chunksize=50_000,\n",
    "    skipbadfiles=True,\n",
    "    # maxchunks=1\n",
    ")\n",
    "\n",
    "preprocess_output = run.preprocess(fileset)\n",
    "\n",
    "# write to disk\n",
    "with open(\"preprocess_output.json\", \"w\") as f:\n",
    "    json.dump(utils.preprocess_to_json(preprocess_output), f)\n",
    "\n",
    "# load from disk\n",
    "with open(\"preprocess_output.json\") as f:\n",
    "    preprocess_output = utils.json_to_preprocess(json.load(f))\n",
    "\n",
    "len(preprocess_output), preprocess_output[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4667b1bf-0ff3-4ccf-93e9-4f4a8e0aa3c7",
   "metadata": {},
   "source": [
    "### processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc78d57d-4fe3-4e11-ab4c-0f0afe60ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analysis(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        self.h = hist.new.Regular(30, 0, 300, label=\"leading electron $p_T$\").\\\n",
    "            StrCat([], name=\"dsid\", growth=True).\\\n",
    "            StrCat([], name=\"variation\", growth=True).\\\n",
    "            Weight()\n",
    "\n",
    "    def process(self, events):\n",
    "        f = uproot.open(events.metadata[\"filename\"])\n",
    "\n",
    "        # this should match existing pre-determined metadata\n",
    "        # sim_type, mc_campaign, dsid, etag = f[\"metadata\"].axes[0]\n",
    "        # assert mc_campaign == events.metadata[\"campaign\"]\n",
    "        # assert dsid == events.metadata[\"dsid\"]\n",
    "\n",
    "        # ensure systematics in schema and in histogram match\n",
    "        # systematics_from_hist = list(f[\"listOfSystematics\"].axes[0])\n",
    "        # assert sorted(systematics_from_hist) == sorted(events.systematic_names)\n",
    "\n",
    "        if events.metadata[\"dsid\"] != \"data\":\n",
    "            sumw = float(f[f.keys(filter_name=\"CutBookkeeper*NOSYS\")[0]].values()[1])  # initial sum of weights\n",
    "        else:\n",
    "            sumw = 1.0  # for data\n",
    "\n",
    "        for variation in events.systematic_names:\n",
    "            if variation != \"NOSYS\" and \"EG_SCALE_ALL\" not in variation:\n",
    "                continue\n",
    "\n",
    "            cut = events[variation][\"pass\"][\"ejets\"] == 1\n",
    "            weight = (events[variation][cut==1].weight.mc if events.metadata[\"dsid\"] != \"data\" else 1.0) * events.metadata[\"weight_xs\"] * events.metadata[\"lumi\"]\n",
    "            self.h.fill(events[variation][cut==1].el.pt[:, 0] / 1_000, dsid=events.metadata[\"dsid\"], variation=variation, weight=weight)\n",
    "\n",
    "        return {\n",
    "            \"hist\": self.h,\n",
    "            \"meta\": {\n",
    "                \"sumw\": {events.metadata[\"dsid\"]: {(events.metadata[\"fileuuid\"], sumw)}}}  # sumw in a set to avoid summing multiple times per file\n",
    "        }\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        # normalize histograms\n",
    "        # https://topcptoolkit.docs.cern.ch/latest/starting/running_local/#sum-of-weights\n",
    "        for dsid in accumulator[\"hist\"].axes[1]:\n",
    "            norm = 1 / sum([sumw for uuid, sumw in accumulator[\"meta\"][\"sumw\"][dsid]])\n",
    "            accumulator[\"hist\"][:, dsid, :] = np.stack([accumulator[\"hist\"][:, dsid, :].values()*norm, accumulator[\"hist\"][:, dsid, :].variances()*norm**2], axis=-1)\n",
    "\n",
    "\n",
    "out, report = run(preprocess_output, processor_instance=Analysis())\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9575019b-d1a5-4a8e-9d5a-32d0d8bd0919",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"data read: {report[\"bytesread\"] / 1000**3:.2f} GB in {report[\"chunks\"]} chunks\")\n",
    "print(f\"average event rate using \\'processtime\\': {report[\"entries\"] / 1000 / report[\"processtime\"]:.2f} kHz\")\n",
    "print(f\"average data rate using \\'processtime\\': {report[\"bytesread\"] / 1000**3 * 8 / report[\"processtime\"]:.2f} Gbps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982cce52-7f5c-4126-a5cc-6a4dfee70732",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_stack = []\n",
    "labels = []\n",
    "for key in dataset_info:\n",
    "    dsids = []\n",
    "    for container in dataset_info[key]:\n",
    "        dsids.append(container.split(\".\")[1])\n",
    "\n",
    "    dsids = sorted(set(dsids))\n",
    "    print(key, dsids)\n",
    "\n",
    "    if key in [\"data\", \"ttbar_H7\", \"ttbar_hdamp\", \"ttbar_pthard\", \"Wt_DS\", \"Wt_H7\", \"Wt_pthard\"]:\n",
    "        continue  # data drawn separately, skip MC modeling variations\n",
    "\n",
    "    try:\n",
    "        mc_stack.append(out[\"hist\"][:, :, \"NOSYS\"].integrate(\"dsid\", dsids))\n",
    "        labels.append(key)\n",
    "    except KeyError as e:\n",
    "        print(f\"  [ERROR] missing DSID (no files available?): {key}, {dsids}, {e}\")\n",
    "\n",
    "fig, ax1, ax2 = mplhep.data_model(\n",
    "    data_hist=out[\"hist\"][:, \"data\", \"NOSYS\"]*450,\n",
    "    stacked_components=mc_stack,\n",
    "    stacked_labels=labels,\n",
    "    # https://scikit-hep.org/mplhep/gallery/model_with_stacked_and_unstacked_histograms_components/\n",
    "    # unstacked_components=[],\n",
    "    # unstacked_labels=[],\n",
    "    xlabel=out[\"hist\"].axes[0].label,\n",
    "    ylabel=\"Entries\",\n",
    ")\n",
    "\n",
    "mplhep.atlas.label(\"Internal\", ax=ax1, data=True, lumi=f\"{utils.integrated_luminosity(\"\", total=True) / 1000:.0f}\", com=\"13 \\\\ / \\\\ 13.6\")\n",
    "mplhep.mpl_magic(ax=ax1)\n",
    "\n",
    "# compare to e.g. https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/HDBS-2020-11/fig_02a.png"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
