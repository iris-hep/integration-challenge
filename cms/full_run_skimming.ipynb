{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# CMS Z' single-lepton: Skimming\n",
    "\n",
    "This notebook extends the full_run workflow to focus on skimming: filtering events and saving to disk in a configurable output format. Supports Parquet on S3, ROOT TTree on XRootD, and ROOT RNTuple on XRootD.\n",
    "\n",
    "A verification cell at the end reads back all skimmed files and checks event counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Workflow Overview\n",
    "\n",
    "1. Setup Python path for intccms package\n",
    "2. Install dependencies and register modules for cloud pickle\n",
    "3. Acquire Dask client from AF environment\n",
    "4. Configure analysis parameters (skimming mode)\n",
    "5. Configure skimming output (format, destination)\n",
    "6. Run metadata extraction (`coffea` preprocessing)\n",
    "7. Run processor with coffea.processor.Runner\n",
    "8. Verify skimmed output by reading back all files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## AF flag\n",
    "We might want to run this code on different facilities, which may each have their own limitations or require different dask client setups. To make it easy to switch between facilities, just set the `AF` variable to the one of your choice. If your `AF` does not exist yet, you can introduce it in this notebook in the relevant sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "AF=\"coffeacasa-condor\" # options currently supported: [coffeacasa-condor, coffeacasa-gateway, purdue-af]\n",
    "AUTO_CLOSE_CLIENT=False # the client setup is done with a contextmanager -- this flag decides if we automatically close the client as we exit the manager. If False, you handle closing manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Imports and dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### The intccms package\n",
    "The CMS implementation of the integration challenge is set in a package-like structure, which means we hae to add the source code to the python path. The package is referred to as `intccms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Python path to include intccms package\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to Python path\n",
    "repo_root = Path.cwd()\n",
    "src_dir = repo_root / \"src\"\n",
    "examples_dir = repo_root\n",
    "if str(src_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(src_dir))\n",
    "if str(examples_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(examples_dir))\n",
    "print(f\"\\u2705 Added {src_dir} to Python path\")\n",
    "print(f\"\\u2705 Added {examples_dir} to Python path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Installig extra dependencies\n",
    "The `intccms` package requires `omegaconf` and `roastcoffea`, which is not by default on an AF. `roastcoffea` is a tool developed while working on this project and it provides an API to extract metrics from coffea-processor workflows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import omegaconf\n",
    "except ImportError:\n",
    "    print(\"\\u26a0\\ufe0f omegaconf not found, installing...\")\n",
    "    ! pip install omegaconf;\n",
    "\n",
    "try:\n",
    "    import roastcoffea\n",
    "except ImportError:\n",
    "    print(\"\\u26a0\\ufe0f roastcoffea not found, installing...\")\n",
    "    ! pip install roastcoffea;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Alternative coffea version\n",
    "In some cases, we might need to install our own `coffea` version which is not on the AF. For example, when testing a new feature or using a recently realased version with a fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "COFFEA_VERSION = \"2025.12.0\"\n",
    "COFFEA_PIP = f\"coffea=={COFFEA_VERSION}\" if \"git\" not in COFFEA_VERSION else COFFEA_VERSION\n",
    "\n",
    "! pip install $COFFEA_PIP ;\n",
    "\n",
    "# Pip-installable dependencies to install on workers\n",
    "WORKER_DEPENDENCIES = [COFFEA_PIP, \"roastcoffea==0.1.2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Imports from stdlib and other libraries\n",
    "\n",
    "In this notebook we use `dask` and `coffea`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "import cloudpickle\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "\n",
    "from coffea.processor import DaskExecutor, IterativeExecutor\n",
    "from coffea.nanoevents import NanoAODSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Imports from intccms and other integration-challenge specific tooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intccms\n",
    "from intccms.schema import Config, load_config_with_restricted_cli\n",
    "from intccms.skimming.io import get_reader\n",
    "from intccms.utils.output import OutputDirectoryManager\n",
    "from intccms.utils.tools import load_dotenv\n",
    "from intccms.metadata_extractor import DatasetMetadataManager\n",
    "from intccms.datasets import DatasetManager\n",
    "from intccms.analysis import run_processor_workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Registering packages with cloudpickle\n",
    "The intccms cannot be installed on the workers via `pip`, and the configuration files are in python modules which also cannot be installed on the workers. So we need to register them with `cloudpickle` to allow dask to serialize them and send them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intccms\n",
    "import example_cms\n",
    "\n",
    "# Register modules for cloud pickle\n",
    "cloudpickle.register_pickle_by_value(intccms)\n",
    "cloudpickle.register_pickle_by_value(example_cms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Dask client setup\n",
    "\n",
    "This notebook uses the `DaskExecutor` from `coffea` to distribute the task graph on the AF. The client setup varies in different facilities, so we implement a function which returns the correct client. The function does so by providing a context manager, within which the client is alive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from intccms.utils.dask_client import acquire_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Configuration Setup\n",
    "\n",
    "The CMS analysis implementation is configurable via python modules, which we have to import. For this notebook, the configuration files are found in `example_cms/configs/`. You can modify the modules in this directory manually, or you can dynamically change settings using python dictionary manipulation. Below are some settings of interest that you might want to tune when you are testing your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intccms configuration import\n",
    "from example_cms.configs.configuration import config as original_config\n",
    "\n",
    "# Create a deepcopy that we can manipulate\n",
    "config = copy.deepcopy(original_config)\n",
    "\n",
    "# Limit files for testing\n",
    "config[\"datasets\"][\"max_files\"] = None # None would run over all availale files\n",
    "\n",
    "# Use local output directory\n",
    "config[\"general\"][\"output_dir\"] = \"example_cms/outputs/\"\n",
    "\n",
    "# Preprocessing (coffea) can be executed once and results loaded\n",
    "config[\"general\"][\"run_metadata_generation\"] = False # If True, run analysis pre-processing\n",
    "\n",
    "# Processer = Skimming (filter and save) + Analysis\n",
    "config[\"general\"][\"run_processor\"] = True  # If True, the coffea processor is executed\n",
    "config[\"general\"][\"run_analysis\"] = False # Skimming only, no analysis\n",
    "config[\"general\"][\"save_skimmed_output\"] = True  # Save skimmed events to disk\n",
    "config[\"general\"][\"use_skimmed_input\"] = False  # Use skimmed events\n",
    "\n",
    "# Analysis = Systematics + histogramming + statsitics\n",
    "config[\"general\"][\"run_histogramming\"] = False\n",
    "config[\"general\"][\"run_systematics\"] = False\n",
    "config[\"general\"][\"run_statistics\"] = False\n",
    "\n",
    "# Datasets to process, by default this is all datasets\n",
    "#config[\"general\"][\"processes\"] = [\"data\"] \n",
    "\n",
    "cli_args = [] # the code can be ran from CLI, but we don't care here\n",
    "full_config = load_config_with_restricted_cli(config, cli_args)\n",
    "\n",
    "# Validated config gives us a dictionary object with all settings checked to be safe with pydantic\n",
    "validated_config = Config(**full_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Skimming Output Configuration\n",
    "\n",
    "Override the skimming output settings to test different backends.\n",
    "\n",
    "| Backend | `OUTPUT_FORMAT` | `OUTPUT_DIR` example |\n",
    "|---------|----------------|---------------------|\n",
    "| Parquet on S3 | `\"parquet\"` | `\"s3:///skim_out\"` |\n",
    "| ROOT TTree on XRootD | `\"ttree\"` | `\"root://xrootd-local.unl.edu:1094//store/user/maly/skim_out/\"` |\n",
    "| ROOT RNTuple on XRootD | `\"rntuple\"` | `\"root://xrootd-local.unl.edu:1094//store/user/maly/skim_out/\"` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options: \"parquet\", \"ttree\", \"rntuple\"\n",
    "OUTPUT_FORMAT = \"rntuple\" #\"ttree\" #\"parquet\"\n",
    "OUTPUT_DIR = \"root://xrootd-local.unl.edu:1094//store/user/maly/TEST_180226_SKIM_RNTUP/\" # \"s3:///TEST_180226_SKIM\"\n",
    "\n",
    "# S3 settings (only used when OUTPUT_DIR starts with \"s3://\")\n",
    "S3_ENDPOINT = \"https://red-s3.unl.edu/cmsaf-test-oshadura\"\n",
    "\n",
    "# --- Derived configuration (no need to edit below) ---\n",
    "\n",
    "to_kwargs = {}\n",
    "from_kwargs = {}\n",
    "PROPAGATE_AWS = False\n",
    "\n",
    "if OUTPUT_DIR.startswith(\"s3://\"):\n",
    "    load_dotenv()\n",
    "    storage_options = {\n",
    "        \"key\": os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        \"secret\": os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        \"client_kwargs\": {\"endpoint_url\": S3_ENDPOINT},\n",
    "    }\n",
    "    to_kwargs[\"storage_options\"] = storage_options\n",
    "    to_kwargs[\"compression\"] = \"zstd\"\n",
    "    from_kwargs[\"storage_options\"] = storage_options\n",
    "    PROPAGATE_AWS = True\n",
    "\n",
    "validated_config.preprocess.skimming.output.format = OUTPUT_FORMAT\n",
    "validated_config.preprocess.skimming.output.output_dir = OUTPUT_DIR\n",
    "validated_config.preprocess.skimming.output.to_kwargs = to_kwargs\n",
    "validated_config.preprocess.skimming.output.from_kwargs = from_kwargs\n",
    "\n",
    "print(f\"Output format: {OUTPUT_FORMAT}\")\n",
    "print(f\"Output dir:    {OUTPUT_DIR}\")\n",
    "if to_kwargs:\n",
    "    print(f\"Writer kwargs: {to_kwargs}\")\n",
    "if PROPAGATE_AWS:\n",
    "    print(\"AWS credentials will be propagated to workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Running the Workflow\n",
    "\n",
    "Running the CMS integration challenge workflow is split into a few steps, with a modular design that allows us flexibility. The steps are:\n",
    "\n",
    "1. Setting up output directories\n",
    "2. Building an input dataset manager\n",
    "3. Running or loading the coffea preprocessing\n",
    "4. Run the coffea processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Output manager setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_manager = OutputDirectoryManager(\n",
    "    root_output_dir=validated_config.general.output_dir,\n",
    "    cache_dir=validated_config.general.cache_dir,\n",
    "    metadata_dir=validated_config.general.metadata_dir,\n",
    "    skimmed_dir=validated_config.general.skimmed_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Configure Data Redirector (Optional)\n",
    "\n",
    "Override the redirector in the config for accessing dataset files. Useful for testing different storage backends. You can also change this in `example_cms/configs/skim.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override redirector for all datasets\n",
    "# Examples:\n",
    "#   \"root://xcache/\"                    \n",
    "#   \"root://cmsxrootd.fnal.gov/\"\n",
    "#   \"root://cms-xrd-global.cern.ch/\"\n",
    "REDIRECTOR = \"root://xcache/\"  # Change this to use a different redirector\n",
    "\n",
    "print(f\"Initial redirector  {validated_config.datasets.datasets[0].name}: {validated_config.datasets.datasets[0].redirector}\")\n",
    "\n",
    "# Apply to all datasets in config\n",
    "for dataset in validated_config.datasets.datasets:\n",
    " dataset.redirector = REDIRECTOR\n",
    "\n",
    "print(f\"Redirector set to: {REDIRECTOR}\")\n",
    "\n",
    "# Verify the change\n",
    "print(f\"New redirector:  {validated_config.datasets.datasets[0].name}: {validated_config.datasets.datasets[0].redirector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Input dataset manager setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_manager = DatasetManager(validated_config.datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Coffea preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_generator = DatasetMetadataManager(\n",
    "  dataset_manager=dataset_manager,\n",
    "  output_manager=output_manager,\n",
    "  config=validated_config,\n",
    ")\n",
    "\n",
    "if metadata_generator.generate_metadata:\n",
    "  with acquire_client(AF, close_after=AUTO_CLOSE_CLIENT, pip_packages=WORKER_DEPENDENCIES) as (client, cluster):\n",
    "      metadata_generator.run(executor=DaskExecutor(client=client))\n",
    "else:\n",
    "  metadata_generator.run()  # No client needed\n",
    "\n",
    "# Build metadata lookup and extract workitems\n",
    "metadata_lookup = metadata_generator.build_metadata_lookup()\n",
    "workitems = metadata_generator.workitems\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Run processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run processor workflow\n",
    "from intccms.analysis.processor import UnifiedProcessor\n",
    "\n",
    "with acquire_client(AF, close_after=AUTO_CLOSE_CLIENT, pip_packages=WORKER_DEPENDENCIES, propagate_aws_env=PROPAGATE_AWS) as (client, cluster):\n",
    "    # Create processor instance for MetricsCollector\n",
    "    unified_processor = UnifiedProcessor(\n",
    "        config=validated_config,\n",
    "        output_manager=output_manager,\n",
    "        metadata_lookup=metadata_lookup,\n",
    "    )\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    output, report = run_processor_workflow(\n",
    "        config=validated_config,\n",
    "        output_manager=output_manager,\n",
    "        metadata_lookup=metadata_lookup,\n",
    "        workitems=workitems,\n",
    "        executor=DaskExecutor(client=client, treereduction=8, retries=0),\n",
    "        schema=NanoAODSchema,\n",
    "    )\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "\n",
    "print(f\"Processor workflow complete in {t1-t0:.1f} seconds!\")\n",
    "\n",
    "# Print summary\n",
    "print(f\"Total events processed: {output.get('processed_events', 0):,}\")\n",
    "print(f\"Events after skim: {output.get('skimmed_events', 0):,}\")\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "Read back all skimmed files and verify event counts match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from intccms.skimming.fileset_manager import FilesetManager\n",
    "\n",
    "# skim_output = validated_config.preprocess.skimming.output\n",
    "# fileset_manager = FilesetManager(\n",
    "#     skimmed_dir=output_manager.skimmed_dir,\n",
    "#     format=skim_output.format,\n",
    "# )\n",
    "\n",
    "# datasets = list(set(md[\"dataset\"] for md in metadata_lookup.values()))\n",
    "# fileset = fileset_manager.build_fileset(datasets)\n",
    "\n",
    "# print(fileset)\n",
    "\n",
    "# reader = get_reader(skim_output.format)\n",
    "# reader_kwargs = dict(skim_output.from_kwargs) if skim_output.from_kwargs else {}\n",
    "# tree_name = validated_config.preprocess.skimming.tree_name \n",
    "\n",
    "# total_readback = 0\n",
    "# for dataset_name, entry in fileset.items():\n",
    "#     for path in entry[\"files\"]:\n",
    "#         events = reader.read(path, tree_name=tree_name, **reader_kwargs)\n",
    "#         n = len(events)\n",
    "#         total_readback += n\n",
    "#         print(f\"  [{dataset_name}] {path.split('/')[-1]}: {n:,} events\")\n",
    "\n",
    "# print(f\"\\nTotal read back:  {total_readback:,}\")\n",
    "# print(f\"Total from skim:  {output.get('skimmed_events', 0):,}\")\n",
    "# assert total_readback == output.get(\"skimmed_events\", 0), \\\n",
    "#     f\"Event count mismatch! readback={total_readback} vs skim={output.get('skimmed_events', 0)}\"\n",
    "# print(\"Verification passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Second pass: Analysis on skimmed files\n",
    "\n",
    "Re-run the processor using the skimmed fileset (`use_skimmed_input=True`). This skips re-reading original NanoAOD and loads the smaller skimmed files instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconfigure for analysis on skimmed input\n",
    "validated_config.general.use_skimmed_input = True\n",
    "validated_config.general.save_skimmed_output = False\n",
    "validated_config.general.run_analysis = True\n",
    "validated_config.general.run_histogramming = True\n",
    "validated_config.general.run_systematics = True\n",
    "\n",
    "with acquire_client(AF, close_after=AUTO_CLOSE_CLIENT, pip_packages=WORKER_DEPENDENCIES, propagate_aws_env=PROPAGATE_AWS) as (client, cluster):\n",
    "    t0 = time.perf_counter()\n",
    "    analysis_output, analysis_report = run_processor_workflow(\n",
    "        config=validated_config,\n",
    "        output_manager=output_manager,\n",
    "        metadata_lookup=metadata_lookup,\n",
    "        workitems=None,  # Not needed â€” fileset built from manifests\n",
    "        executor=DaskExecutor(client=client, treereduction=8, retries=0),\n",
    "        schema=NanoAODSchema,\n",
    "    )\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "print(f\"Analysis on skimmed files complete in {t1-t0:.1f} seconds!\")\n",
    "print(f\"Total events processed: {analysis_output.get('processed_events', 0):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "if AF != \"iterative\" and not AUTO_CLOSE_CLIENT:\n",
    "    client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
