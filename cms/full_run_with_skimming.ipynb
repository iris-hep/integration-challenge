{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMS Z' single-lepton: Skimming workflows\n",
    "\n",
    "This notebook demonstrates the four skimming workflow modes supported by the processor,\n",
    "with performance metrics collected via [roastcoffea](https://github.com/MoAly98/roastcoffea).\n",
    "\n",
    "| Mode | `save_skimmed_output` | `run_analysis` | Description |\n",
    "|------|-----------------------|----------------|-------------|\n",
    "| **1. Skim + Analysis** | `True` | `True` | Skim events to disk **and** run histogramming in one pass |\n",
    "| **2. Analysis only** | `False` | `True` | Apply skim filter on-the-fly, no files saved |\n",
    "| **3. Skim only** | `True` | `False` | Save skimmed files to disk, skip histogramming |\n",
    "| **4. Analysis on skimmed** | `False` | `True` | Read previously skimmed files as input (`use_skimmed_input=True`) |\n",
    "\n",
    "Each mode cell is independent â€” run any subset in any order (Mode 4 requires skimmed files from Mode 1 or 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AF flag\n",
    "Set the analysis facility. Each AF has its own Dask client setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AF = \"coffeacasa-condor\"  # options: [coffeacasa-condor, coffeacasa-gateway, purdue-af]\n",
    "AUTO_CLOSE_CLIENT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Python path to include intccms package\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "repo_root = Path.cwd()\n",
    "src_dir = repo_root / \"src\"\n",
    "examples_dir = repo_root\n",
    "if str(src_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(src_dir))\n",
    "if str(examples_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(examples_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import omegaconf\n",
    "except ImportError:\n",
    "    print(\"omegaconf not found, installing...\")\n",
    "    ! pip install omegaconf;\n",
    "\n",
    "try:\n",
    "    import roastcoffea\n",
    "except ImportError:\n",
    "    print(\"roastcoffea not found, installing...\")\n",
    "    ! pip install roastcoffea;\n",
    "    import roastcoffea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COFFEA_VERSION = \"2025.12.0\"\n",
    "COFFEA_PIP = f\"coffea=={COFFEA_VERSION}\" if \"git\" not in COFFEA_VERSION else COFFEA_VERSION\n",
    "\n",
    "! pip install $COFFEA_PIP ;\n",
    "\n",
    "WORKER_DEPENDENCIES = [COFFEA_PIP, \"roastcoffea==0.1.2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "\n",
    "import fsspec\n",
    "from coffea.processor import DaskExecutor\n",
    "from coffea.nanoevents import NanoAODSchema\n",
    "\n",
    "from intccms.schema import Config, load_config_with_restricted_cli\n",
    "from intccms.utils.output import OutputDirectoryManager\n",
    "from intccms.utils.tools import load_dotenv\n",
    "from intccms.utils.dask_client import acquire_client\n",
    "from intccms.metadata_extractor import DatasetMetadataManager\n",
    "from intccms.datasets import DatasetManager\n",
    "from intccms.analysis import run_processor_workflow\n",
    "from intccms.analysis.processor import UnifiedProcessor\n",
    "\n",
    "from roastcoffea import MetricsCollector\n",
    "from roastcoffea.export.reporter import (\n",
    "    format_throughput_table,\n",
    "    format_event_processing_table,\n",
    "    format_timing_table,\n",
    ")\n",
    "from roastcoffea.visualization.plots import (\n",
    "    plot_worker_count_timeline,\n",
    "    plot_throughput_timeline,\n",
    "    plot_runtime_distribution,\n",
    ")\n",
    "from rich.console import Console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import intccms\n",
    "import example_cms\n",
    "\n",
    "cloudpickle.register_pickle_by_value(intccms)\n",
    "cloudpickle.register_pickle_by_value(example_cms)\n",
    "cloudpickle.register_pickle_by_value(roastcoffea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from example_cms.configs.configuration import config as original_config\n",
    "\n",
    "config = copy.deepcopy(original_config)\n",
    "\n",
    "config[\"datasets\"][\"max_files\"] = None\n",
    "config[\"general\"][\"output_dir\"] = \"example_cms/outputs/\"\n",
    "config[\"general\"][\"run_metadata_generation\"] = False\n",
    "config[\"general\"][\"run_processor\"] = True\n",
    "\n",
    "# Defaults -- each mode cell overrides these\n",
    "config[\"general\"][\"run_analysis\"] = False\n",
    "config[\"general\"][\"save_skimmed_output\"] = False\n",
    "config[\"general\"][\"use_skimmed_input\"] = False\n",
    "config[\"general\"][\"run_histogramming\"] = False\n",
    "config[\"general\"][\"run_systematics\"] = False\n",
    "config[\"general\"][\"run_statistics\"] = False\n",
    "\n",
    "cli_args = []\n",
    "full_config = load_config_with_restricted_cli(config, cli_args)\n",
    "base_config = Config(**full_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skimming output configuration\n",
    "\n",
    "Configure the output format and destination for skimmed files.\n",
    "Secrets (AWS keys) are loaded from an untracked `.env` file via `load_dotenv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment one of the three presets below to select the output backend.\n",
    "# Each preset sets OUTPUT_FORMAT and OUTPUT_DIR; the rest is derived automatically.\n",
    "\n",
    "# --- Parquet on S3 ---\n",
    "OUTPUT_FORMAT = \"parquet\"\n",
    "OUTPUT_DIR = \"s3:///skim_out\"\n",
    "S3_ENDPOINT = \"https://red-s3.unl.edu/cmsaf-test-oshadura\"\n",
    "\n",
    "# --- ROOT TTree on XRootD ---\n",
    "# OUTPUT_FORMAT = \"ttree\"\n",
    "# OUTPUT_DIR = \"root://xrootd-local.unl.edu:1094//store/user/maly/skim_ttree/\"\n",
    "\n",
    "# --- ROOT RNTuple on XRootD ---\n",
    "# OUTPUT_FORMAT = \"rntuple\"\n",
    "# OUTPUT_DIR = \"root://xrootd-local.unl.edu:1094//store/user/maly/skim_rntuple/\"\n",
    "\n",
    "# --- Derived configuration (no need to edit below) ---\n",
    "\n",
    "to_kwargs = {}\n",
    "from_kwargs = {}\n",
    "PROPAGATE_AWS = False\n",
    "\n",
    "if OUTPUT_DIR.startswith(\"s3://\"):\n",
    "    load_dotenv()\n",
    "    storage_options = {\n",
    "        \"key\": os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        \"secret\": os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        \"client_kwargs\": {\"endpoint_url\": S3_ENDPOINT},\n",
    "    }\n",
    "    to_kwargs[\"storage_options\"] = storage_options\n",
    "    to_kwargs[\"compression\"] = \"zstd\"\n",
    "    from_kwargs[\"storage_options\"] = storage_options\n",
    "    PROPAGATE_AWS = True\n",
    "\n",
    "base_config.preprocess.skimming.output.format = OUTPUT_FORMAT\n",
    "base_config.preprocess.skimming.output.output_dir = OUTPUT_DIR\n",
    "base_config.preprocess.skimming.output.to_kwargs = to_kwargs\n",
    "base_config.preprocess.skimming.output.from_kwargs = from_kwargs\n",
    "\n",
    "print(f\"Output format: {OUTPUT_FORMAT}\")\n",
    "print(f\"Output dir:    {OUTPUT_DIR}\")\n",
    "if PROPAGATE_AWS:\n",
    "    print(\"AWS credentials will be propagated to workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data redirector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDIRECTOR = \"root://xcache/\"\n",
    "\n",
    "for dataset in base_config.datasets.datasets:\n",
    "    dataset.redirector = REDIRECTOR\n",
    "\n",
    "print(f\"Redirector set to: {REDIRECTOR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output manager, dataset manager, and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_manager = OutputDirectoryManager(\n",
    "    root_output_dir=base_config.general.output_dir,\n",
    "    cache_dir=base_config.general.cache_dir,\n",
    "    metadata_dir=base_config.general.metadata_dir,\n",
    "    skimmed_dir=base_config.general.skimmed_dir,\n",
    ")\n",
    "\n",
    "dataset_manager = DatasetManager(base_config.datasets)\n",
    "\n",
    "metadata_generator = DatasetMetadataManager(\n",
    "    dataset_manager=dataset_manager,\n",
    "    output_manager=output_manager,\n",
    "    config=base_config,\n",
    ")\n",
    "\n",
    "if metadata_generator.generate_metadata:\n",
    "    with acquire_client(AF, close_after=AUTO_CLOSE_CLIENT, pip_packages=WORKER_DEPENDENCIES) as (client, cluster):\n",
    "        metadata_generator.run(executor=DaskExecutor(client=client))\n",
    "else:\n",
    "    metadata_generator.run()\n",
    "\n",
    "metadata_lookup = metadata_generator.build_metadata_lookup()\n",
    "workitems = metadata_generator.workitems\n",
    "print(f\"Generated {len(workitems)} workitems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared results dict for cross-mode comparison\n",
    "results = {}\n",
    "console = Console()\n",
    "\n",
    "\n",
    "def measure_skimmed_size(output, skim_output_config):\n",
    "    \"\"\"Stat all output files from manifest_entries to get total skimmed bytes.\"\"\"\n",
    "    entries = output.get(\"manifest_entries\", [])\n",
    "    if not entries:\n",
    "        return 0\n",
    "\n",
    "    # Build storage_options for fsspec from the skim output config\n",
    "    so = dict(skim_output_config.from_kwargs) if skim_output_config.from_kwargs else {}\n",
    "\n",
    "    total_bytes = 0\n",
    "    for entry in entries:\n",
    "        path = entry[\"output_file\"]\n",
    "        try:\n",
    "            of = fsspec.open(path, **(so.get(\"storage_options\", so)))\n",
    "            info = of.fs.info(of.path)\n",
    "            total_bytes += info.get(\"size\", 0)\n",
    "        except Exception as e:\n",
    "            print(f\"  Could not stat {path}: {e}\")\n",
    "    return total_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Mode 1: Skim + Analysis\n",
    "\n",
    "Skim events to disk **and** run the full analysis (histogramming + systematics) in a single pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = copy.deepcopy(base_config)\n",
    "cfg.general.save_skimmed_output = True\n",
    "cfg.general.run_analysis = True\n",
    "cfg.general.run_histogramming = True\n",
    "cfg.general.run_systematics = True\n",
    "cfg.general.use_skimmed_input = False\n",
    "\n",
    "with acquire_client(AF, close_after=AUTO_CLOSE_CLIENT, pip_packages=WORKER_DEPENDENCIES, propagate_aws_env=PROPAGATE_AWS) as (client, cluster):\n",
    "    processor = UnifiedProcessor(\n",
    "        config=cfg, output_manager=output_manager, metadata_lookup=metadata_lookup,\n",
    "    )\n",
    "    with MetricsCollector(\n",
    "        client=client, processor_instance=processor,\n",
    "        track_workers=True, worker_tracking_interval=1.0,\n",
    "    ) as collector:\n",
    "        t0 = time.perf_counter()\n",
    "        output, report = run_processor_workflow(\n",
    "            config=cfg, output_manager=output_manager,\n",
    "            metadata_lookup=metadata_lookup, workitems=workitems,\n",
    "            executor=DaskExecutor(client=client, treereduction=8, retries=0),\n",
    "            schema=NanoAODSchema,\n",
    "        )\n",
    "        t1 = time.perf_counter()\n",
    "        collector.extract_metrics_from_output(output)\n",
    "        collector.set_coffea_report(report)\n",
    "\n",
    "    metrics = collector.get_metrics()\n",
    "    skimmed_bytes = measure_skimmed_size(output, cfg.preprocess.skimming.output)\n",
    "    save_time = metrics.get(\"sections\", {}).get(\"save_skimmed\", {}).get(\"total_duration\", 0)\n",
    "\n",
    "    results[\"skim_and_analysis\"] = {\n",
    "        \"metrics\": metrics,\n",
    "        \"tracking_data\": collector.tracking_data,\n",
    "        \"wall_time\": t1 - t0,\n",
    "        \"processed_events\": output.get(\"processed_events\", 0),\n",
    "        \"skimmed_events\": output.get(\"skimmed_events\", 0),\n",
    "        \"skimmed_bytes\": skimmed_bytes,\n",
    "        \"save_skimmed_time\": save_time,\n",
    "    }\n",
    "\n",
    "print(f\"Mode 1 complete in {t1-t0:.1f}s\")\n",
    "print(f\"  Processed: {output.get('processed_events', 0):,}\")\n",
    "print(f\"  Skimmed:   {output.get('skimmed_events', 0):,}\")\n",
    "print(f\"  Save time: {save_time:.1f}s\")\n",
    "print(f\"  File size: {skimmed_bytes / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Mode 2: Analysis only (no skim)\n",
    "\n",
    "Apply the skim filter on-the-fly and run the analysis. No skimmed files are saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = copy.deepcopy(base_config)\n",
    "cfg.general.save_skimmed_output = False\n",
    "cfg.general.run_analysis = True\n",
    "cfg.general.run_histogramming = True\n",
    "cfg.general.run_systematics = True\n",
    "cfg.general.use_skimmed_input = False\n",
    "\n",
    "with acquire_client(AF, close_after=AUTO_CLOSE_CLIENT, pip_packages=WORKER_DEPENDENCIES, propagate_aws_env=PROPAGATE_AWS) as (client, cluster):\n",
    "    processor = UnifiedProcessor(\n",
    "        config=cfg, output_manager=output_manager, metadata_lookup=metadata_lookup,\n",
    "    )\n",
    "    with MetricsCollector(\n",
    "        client=client, processor_instance=processor,\n",
    "        track_workers=True, worker_tracking_interval=1.0,\n",
    "    ) as collector:\n",
    "        t0 = time.perf_counter()\n",
    "        output, report = run_processor_workflow(\n",
    "            config=cfg, output_manager=output_manager,\n",
    "            metadata_lookup=metadata_lookup, workitems=workitems,\n",
    "            executor=DaskExecutor(client=client, treereduction=8, retries=0),\n",
    "            schema=NanoAODSchema,\n",
    "        )\n",
    "        t1 = time.perf_counter()\n",
    "        collector.extract_metrics_from_output(output)\n",
    "        collector.set_coffea_report(report)\n",
    "\n",
    "    results[\"analysis_only\"] = {\n",
    "        \"metrics\": collector.get_metrics(),\n",
    "        \"tracking_data\": collector.tracking_data,\n",
    "        \"wall_time\": t1 - t0,\n",
    "        \"processed_events\": output.get(\"processed_events\", 0),\n",
    "        \"skimmed_events\": output.get(\"skimmed_events\", 0),\n",
    "        \"skimmed_bytes\": 0,\n",
    "        \"save_skimmed_time\": 0,\n",
    "    }\n",
    "\n",
    "print(f\"Mode 2 complete in {t1-t0:.1f}s\")\n",
    "print(f\"  Processed: {output.get('processed_events', 0):,}\")\n",
    "print(f\"  Skimmed:   {output.get('skimmed_events', 0):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Mode 3: Skim only (no analysis)\n",
    "\n",
    "Save skimmed files to disk without running histogramming or systematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = copy.deepcopy(base_config)\n",
    "cfg.general.save_skimmed_output = True\n",
    "cfg.general.run_analysis = False\n",
    "cfg.general.run_histogramming = False\n",
    "cfg.general.run_systematics = False\n",
    "cfg.general.use_skimmed_input = False\n",
    "\n",
    "with acquire_client(AF, close_after=AUTO_CLOSE_CLIENT, pip_packages=WORKER_DEPENDENCIES, propagate_aws_env=PROPAGATE_AWS) as (client, cluster):\n",
    "    processor = UnifiedProcessor(\n",
    "        config=cfg, output_manager=output_manager, metadata_lookup=metadata_lookup,\n",
    "    )\n",
    "    with MetricsCollector(\n",
    "        client=client, processor_instance=processor,\n",
    "        track_workers=True, worker_tracking_interval=1.0,\n",
    "    ) as collector:\n",
    "        t0 = time.perf_counter()\n",
    "        output, report = run_processor_workflow(\n",
    "            config=cfg, output_manager=output_manager,\n",
    "            metadata_lookup=metadata_lookup, workitems=workitems,\n",
    "            executor=DaskExecutor(client=client, treereduction=8, retries=0),\n",
    "            schema=NanoAODSchema,\n",
    "        )\n",
    "        t1 = time.perf_counter()\n",
    "        collector.extract_metrics_from_output(output)\n",
    "        collector.set_coffea_report(report)\n",
    "\n",
    "    metrics = collector.get_metrics()\n",
    "    skimmed_bytes = measure_skimmed_size(output, cfg.preprocess.skimming.output)\n",
    "    save_time = metrics.get(\"sections\", {}).get(\"save_skimmed\", {}).get(\"total_duration\", 0)\n",
    "\n",
    "    results[\"skim_only\"] = {\n",
    "        \"metrics\": metrics,\n",
    "        \"tracking_data\": collector.tracking_data,\n",
    "        \"wall_time\": t1 - t0,\n",
    "        \"processed_events\": output.get(\"processed_events\", 0),\n",
    "        \"skimmed_events\": output.get(\"skimmed_events\", 0),\n",
    "        \"skimmed_bytes\": skimmed_bytes,\n",
    "        \"save_skimmed_time\": save_time,\n",
    "    }\n",
    "\n",
    "print(f\"Mode 3 complete in {t1-t0:.1f}s\")\n",
    "print(f\"  Processed: {output.get('processed_events', 0):,}\")\n",
    "print(f\"  Skimmed:   {output.get('skimmed_events', 0):,}\")\n",
    "print(f\"  Save time: {save_time:.1f}s\")\n",
    "print(f\"  File size: {skimmed_bytes / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Mode 4: Analysis on skimmed files\n",
    "\n",
    "Read previously skimmed files (from Mode 1 or 3) and run the analysis on them.\n",
    "Requires that skimmed files exist at the configured output location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = copy.deepcopy(base_config)\n",
    "cfg.general.save_skimmed_output = False\n",
    "cfg.general.run_analysis = True\n",
    "cfg.general.run_histogramming = True\n",
    "cfg.general.run_systematics = True\n",
    "cfg.general.use_skimmed_input = True\n",
    "\n",
    "with acquire_client(AF, close_after=AUTO_CLOSE_CLIENT, pip_packages=WORKER_DEPENDENCIES, propagate_aws_env=PROPAGATE_AWS) as (client, cluster):\n",
    "    processor = UnifiedProcessor(\n",
    "        config=cfg, output_manager=output_manager, metadata_lookup=metadata_lookup,\n",
    "    )\n",
    "    with MetricsCollector(\n",
    "        client=client, processor_instance=processor,\n",
    "        track_workers=True, worker_tracking_interval=1.0,\n",
    "    ) as collector:\n",
    "        t0 = time.perf_counter()\n",
    "        output, report = run_processor_workflow(\n",
    "            config=cfg, output_manager=output_manager,\n",
    "            metadata_lookup=metadata_lookup, workitems=None,\n",
    "            executor=DaskExecutor(client=client, treereduction=8, retries=0),\n",
    "            schema=NanoAODSchema,\n",
    "        )\n",
    "        t1 = time.perf_counter()\n",
    "        collector.extract_metrics_from_output(output)\n",
    "        collector.set_coffea_report(report)\n",
    "\n",
    "    results[\"analysis_on_skimmed\"] = {\n",
    "        \"metrics\": collector.get_metrics(),\n",
    "        \"tracking_data\": collector.tracking_data,\n",
    "        \"wall_time\": t1 - t0,\n",
    "        \"processed_events\": output.get(\"processed_events\", 0),\n",
    "        \"skimmed_events\": output.get(\"skimmed_events\", 0),\n",
    "        \"skimmed_bytes\": 0,\n",
    "        \"save_skimmed_time\": 0,\n",
    "    }\n",
    "\n",
    "print(f\"Mode 4 complete in {t1-t0:.1f}s\")\n",
    "print(f\"  Processed: {output.get('processed_events', 0):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Metrics comparison\n",
    "\n",
    "Compare performance across all workflow modes that were executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wall-clock timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'Mode':<25} {'Wall (s)':>10} {'Save (s)':>10} {'Processed':>12} {'Skimmed':>10} {'Size (MB)':>10}\")\n",
    "print(\"-\" * 81)\n",
    "for mode_name, r in results.items():\n",
    "    size_mb = r[\"skimmed_bytes\"] / 1e6 if r[\"skimmed_bytes\"] else 0\n",
    "    print(\n",
    "        f\"{mode_name:<25} {r['wall_time']:>10.1f} {r['save_skimmed_time']:>10.1f} \"\n",
    "        f\"{r['processed_events']:>12,} {r['skimmed_events']:>10,} {size_mb:>10.1f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throughput and timing tables (roastcoffea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode_name, r in results.items():\n",
    "    metrics = r[\"metrics\"]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  {mode_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    print(\"\\nThroughput\")\n",
    "    console.print(format_throughput_table(metrics))\n",
    "\n",
    "    print(\"\\nEvent Processing\")\n",
    "    console.print(format_event_processing_table(metrics))\n",
    "\n",
    "    print(\"\\nTiming Breakdown\")\n",
    "    console.print(format_timing_table(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for mode_name, r in results.items():\n",
    "    metrics = r[\"metrics\"]\n",
    "    tracking_data = r[\"tracking_data\"]\n",
    "\n",
    "    print(f\"\\n--- {mode_name} ---\")\n",
    "\n",
    "    try:\n",
    "        fig, ax = plot_worker_count_timeline(tracking_data)\n",
    "        ax.set_title(f\"{mode_name}: Worker Count\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"  Worker count plot unavailable: {e}\")\n",
    "\n",
    "    try:\n",
    "        fig, ax = plot_throughput_timeline(metrics[\"chunk_info\"], tracking_data)\n",
    "        ax.set_title(f\"{mode_name}: Throughput\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"  Throughput plot unavailable: {e}\")\n",
    "\n",
    "    try:\n",
    "        fig, ax = plot_runtime_distribution(metrics.get(\"raw_chunk_metrics\"))\n",
    "        ax.set_title(f\"{mode_name}: Runtime Distribution\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"  Runtime distribution plot unavailable: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
