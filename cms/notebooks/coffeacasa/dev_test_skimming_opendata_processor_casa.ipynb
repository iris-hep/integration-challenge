{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coffea-Casa Processor-Based Workflow Test\n",
    "\n",
    "This notebook demonstrates the UnifiedProcessor workflow with coffea.processor.Runner on Coffea-Casa, including skimming, analysis, histogramming, and statistics steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Overview\n",
    "\n",
    "1. Setup Python path for intccms package\n",
    "2. Install dependencies and register modules for cloud pickle\n",
    "3. Acquire Dask client from Coffea-Casa environment\n",
    "4. Configure analysis parameters\n",
    "5. Run metadata extraction\n",
    "6. Initialize UnifiedProcessor\n",
    "7. Run processor with coffea.processor.Runner\n",
    "8. Save histograms\n",
    "9. Run statistical analysis (if enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Python path to include intccms package\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to Python path\n",
    "repo_root = Path.cwd().parent\n",
    "src_dir = repo_root / \"src\"\n",
    "if str(src_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(src_dir))\n",
    "\n",
    "print(f\"âœ… Added {src_dir} to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "! pip install omegaconf\n",
    "! pip install coffea==2025.10.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and cloudpickle registration\n",
    "import copy\n",
    "import os\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = \"\"\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = \"\"\n",
    "\n",
    "from dask.distributed import Client, PipInstall\n",
    "from coffea.processor import DaskExecutor\n",
    "from coffea.nanoevents import NanoAODSchema\n",
    "\n",
    "import cloudpickle\n",
    "import intccms\n",
    "import example_opendata\n",
    "\n",
    "# Register modules for cloud pickle\n",
    "cloudpickle.register_pickle_by_value(intccms)\n",
    "cloudpickle.register_pickle_by_value(example_opendata)\n",
    "\n",
    "from example_opendata.configs.configuration import config as original_config\n",
    "from intccms.utils.schema import Config, load_config_with_restricted_cli\n",
    "from intccms.utils.output_manager import OutputDirectoryManager\n",
    "from intccms.metadata_extractor import DatasetMetadataManager\n",
    "from intccms.utils.datasets import DatasetManager\n",
    "from intccms.analysis import run_processor_workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire Dask Client\n",
    "\n",
    "Coffea-Casa provides a shared scheduler. Connect to it and register dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_client():\n",
    "    \"\"\"Acquire Dask client from Coffea-Casa environment.\"\"\"\n",
    "    dependencies = [\"coffea==2025.10.2\"]\n",
    "    client = Client(\"tls://localhost:8786\")\n",
    "    client.register_plugin(PipInstall(packages=dependencies))\n",
    "    cluster = None  # no local cluster in this mode\n",
    "    return client, cluster\n",
    "\n",
    "client, cluster = acquire_client()\n",
    "print(f\"âœ… Connected to Dask scheduler\")\n",
    "print(f\"ğŸ“Š Dashboard: {client.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Setup\n",
    "\n",
    "Configure analysis parameters including which processes to run and output settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration setup\n",
    "config = copy.deepcopy(original_config)\n",
    "\n",
    "# Limit files for testing\n",
    "config[\"datasets\"][\"max_files\"] = None\n",
    "\n",
    "# Use local output directory\n",
    "config[\"general\"][\"output_dir\"] = \"dev/dev_outputs_opendata_processor_casa\"\n",
    "\n",
    "# Configuration flags\n",
    "config[\"general\"][\"read_from_cache\"] = False\n",
    "config[\"general\"][\"run_metadata_generation\"] = True\n",
    "config[\"general\"][\"run_processor\"] = True  # Set to False to skip processor and load saved histograms\n",
    "config[\"general\"][\"save_skimmed_output\"] = False  # Set to True to save filtered events to disk\n",
    "config[\"general\"][\"run_analysis\"] = True\n",
    "config[\"general\"][\"run_histogramming\"] = True\n",
    "config[\"general\"][\"run_systematics\"] = False\n",
    "config[\"general\"][\"run_statistics\"] = True\n",
    "\n",
    "# Test only signal dataset\n",
    "config[\"general\"][\"processes\"] = [\"signal\"]\n",
    "\n",
    "cli_args = []\n",
    "full_config = load_config_with_restricted_cli(config, cli_args)\n",
    "validated_config = Config(**full_config)\n",
    "\n",
    "print(f\"âœ… Configuration loaded with max_files={validated_config.datasets.max_files}\")\n",
    "print(f\"   - run_processor: {validated_config.general.run_processor}\")\n",
    "print(f\"   - save_skimmed_output: {validated_config.general.save_skimmed_output}\")\n",
    "print(f\"   - run_analysis: {validated_config.general.run_analysis}\")\n",
    "print(f\"   - run_histogramming: {validated_config.general.run_histogramming}\")\n",
    "print(f\"   - run_systematics: {validated_config.general.run_systematics}\")\n",
    "print(f\"   - run_statistics: {validated_config.general.run_statistics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Complete Workflow\n",
    "\n",
    "Execute the full processor workflow with proper cleanup in a try/finally block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Output Manager Setup\n",
    "    output_manager = OutputDirectoryManager(\n",
    "        root_output_dir=validated_config.general.output_dir,\n",
    "        cache_dir=validated_config.general.cache_dir,\n",
    "        metadata_dir=validated_config.general.metadata_dir,\n",
    "        skimmed_dir=validated_config.general.skimmed_dir\n",
    "    )\n",
    "    print(f\"âœ… Output directory: {output_manager.root_output_dir}\")\n",
    "\n",
    "    # Step 1: Metadata Extraction\n",
    "    print(\"\\nğŸ“‹ Extracting metadata...\")\n",
    "    dataset_manager = DatasetManager(validated_config.datasets)\n",
    "    metadata_generator = DatasetMetadataManager(\n",
    "        dataset_manager=dataset_manager,\n",
    "        output_manager=output_manager,\n",
    "        executor=DaskExecutor(client=client),\n",
    "    )\n",
    "    metadata_generator.run(\n",
    "        generate_metadata=validated_config.general.run_metadata_generation,\n",
    "        processes_filter=validated_config.general.processes if hasattr(validated_config.general, 'processes') else None\n",
    "    )\n",
    "\n",
    "    metadata_lookup = metadata_generator.build_metadata_lookup()\n",
    "    workitems = metadata_generator.workitems\n",
    "\n",
    "    print(f\"âœ… Generated {len(workitems)} workitems\")\n",
    "\n",
    "    # Show first few workitems\n",
    "    print(\"\\nğŸ” Workitem Details (first 5):\")\n",
    "    for i, wi in enumerate(workitems[:5]):\n",
    "        print(f\"  {i}: dataset='{wi.dataset}' process='{wi.usermeta.get('process', 'N/A')}'\")\n",
    "    if len(workitems) > 5:\n",
    "        print(f\"  ... and {len(workitems) - 5} more\")\n",
    "\n",
    "    # Step 2: Run Processor Workflow (or load saved histograms)\n",
    "    print(\"\\nğŸš€ Running processor workflow...\")\n",
    "    output = run_processor_workflow(\n",
    "        config=validated_config,\n",
    "        output_manager=output_manager,\n",
    "        metadata_lookup=metadata_lookup,\n",
    "        workitems=workitems,\n",
    "        executor=DaskExecutor(client=client),\n",
    "        schema=NanoAODSchema,\n",
    "    )\n",
    "    print(\"âœ… Processor workflow complete!\")\n",
    "\n",
    "    # Step 3: Display Results\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ“Š Results:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if validated_config.general.run_processor:\n",
    "        print(f\"ğŸ“Š Total events processed: {output.get('processed_events', 0):,}\")\n",
    "        if 'skimmed_events' in output:\n",
    "            print(f\"âœ‚ï¸  Events after skim: {output.get('skimmed_events', 0):,}\")\n",
    "\n",
    "    # Histograms are auto-saved by processor\n",
    "    if output and \"histograms\" in output:\n",
    "        num_histograms = sum(len(hists) for hists in output[\"histograms\"].values())\n",
    "        print(f\"ğŸ“ˆ Total histograms: {num_histograms}\")\n",
    "        print(f\"ğŸ“ˆ Channels: {list(output['histograms'].keys())}\")\n",
    "        print(f\"âœ… Histograms auto-saved to: {output_manager.get_histograms_dir()}\")\n",
    "        print(f\"   - processor_histograms.pkl (for loading with run_processor=False)\")\n",
    "        print(f\"   - histograms.root (for downstream tools)\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  No histograms produced (run_histogramming may be disabled)\")\n",
    "\n",
    "    # Step 4: Run Statistical Analysis\n",
    "    if validated_config.general.run_statistics and output and \"histograms\" in output:\n",
    "        print(f\"\\nğŸ“ˆ Statistical analysis auto-saved to: {output_manager.get_statistics_dir()}\")\n",
    "        print(f\"   - workspace.json (cabinetry workspace)\")\n",
    "        print(f\"   - Pre-fit and post-fit plots\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
