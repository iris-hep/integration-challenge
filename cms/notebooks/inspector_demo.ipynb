{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data Inspector Demo\n",
    "\n",
    "This notebook demonstrates the `intccms.metrics.inspector` module for characterizing input ROOT files.\n",
    "\n",
    "The inspector allows you to:\n",
    "- Extract metadata from ROOT files (events, file sizes, branch sizes, compression ratios)\n",
    "- Run distributed inspection using Dask\n",
    "- Aggregate statistics across datasets\n",
    "- Create visualizations of input data characteristics\n",
    "\n",
    "**Key feature**: Works directly with DatasetManager - no metadata preprocessing required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nfrom pathlib import Path\n\n# Add project root to path\nproject_root = Path.cwd().parent\nif str(project_root) not in sys.path:\n    sys.path.insert(0, str(project_root))\n\nfrom dask.distributed import Client, LocalCluster\nimport matplotlib.pyplot as plt\n\nfrom intccms.datasets import DatasetManager\nfrom intccms.metrics.inspector import (\n    extract_files_from_dataset_manager,\n    get_dataset_file_counts,\n    inspect_dataset_distributed,\n    aggregate_statistics,\n    group_by_dataset,\n    compute_dataset_statistics,\n    compute_compression_stats,\n    format_error_summary,\n    plot,\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset Configuration\n",
    "\n",
    "Load the dataset configuration from `example_cms/configs/skim.py`.\n",
    "This uses the same configuration as your processing workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset configuration\n",
    "from example_cms.configs.skim import datasets_config\n",
    "\n",
    "# Create DatasetManager\n",
    "dm = DatasetManager(datasets_config)\n",
    "\n",
    "print(\"Available datasets:\")\n",
    "for name in dm.datasets.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Quick File Count Summary\n",
    "\n",
    "Get a quick count of files per dataset without full inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_counts = get_dataset_file_counts(dm)\n",
    "\n",
    "print(\"\\nFile counts per dataset:\")\n",
    "for dataset, count in file_counts.items():\n",
    "    print(f\"  {dataset}: {count} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract Files for Inspection\n",
    "\n",
    "Extract file paths from DatasetManager. You can:\n",
    "- Inspect all datasets\n",
    "- Inspect specific processes\n",
    "- Limit files per process (useful for quick sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Sample first 5 files per dataset for quick testing\n",
    "file_list, dataset_map = extract_files_from_dataset_manager(\n",
    "    dm,\n",
    "    max_files_per_process=5,\n",
    ")\n",
    "\n",
    "# Option B: Inspect specific datasets\n",
    "# file_list, dataset_map = extract_files_from_dataset_manager(\n",
    "#     dm,\n",
    "#     processes=[\"signal\", \"ttbar_semilep\"],\n",
    "#     max_files_per_process=10,\n",
    "# )\n",
    "\n",
    "# Option C: Inspect all files (can be slow for large datasets!)\n",
    "# file_list, dataset_map = extract_files_from_dataset_manager(dm)\n",
    "\n",
    "print(f\"\\nExtracted {len(file_list)} files for inspection\")\n",
    "print(f\"Example file: {file_list[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Distributed Inspection with Dask\n",
    "\n",
    "Run distributed file inspection using Dask.\n",
    "This extracts metadata from all files in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a local Dask cluster\n",
    "cluster = LocalCluster(n_workers=4, threads_per_worker=1, processes=True)\n",
    "client = Client(cluster)\n",
    "\n",
    "print(f\"Dask dashboard: {client.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run distributed inspection with error handling\n# Note: max_branches limits the number of branches inspected per file for faster results\nresults, errors = inspect_dataset_distributed(\n    client,\n    file_list,\n    max_branches=100,  # Limit to first 100 branches for speed\n)\n\n# Print formatted error summary\nprint(format_error_summary(errors))\n\nif results:\n    print(f\"\\n=== Example Result ===\")\n    print(f\"  File: {results[0]['filepath']}\")\n    print(f\"  Events: {results[0]['num_events']:,}\")\n    print(f\"  Branches: {results[0]['num_branches']}\")\n    print(f\"  File size: {results[0]['file_size_bytes'] / 1024**2:.1f} MB\")\nelse:\n    print(\"\\nNo files were successfully inspected!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Aggregate Statistics\n",
    "\n",
    "Compute aggregate statistics across all inspected files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Fetch File Sizes from Rucio\n",
    "\n",
    "Use the inspector's Rucio helper to retrieve authoritative file sizes. This\n",
    "requires a valid Rucio environment (credentials and network access). If the\n",
    "lookup fails, the notebook continues with locally derived statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intccms.metrics.inspector import rucio as inspector_rucio\n",
    "from rich.console import Console\n",
    "\n",
    "size_summary = None\n",
    "try:\n",
    "    size_summary = inspector_rucio.fetch_file_sizes(\n",
    "        dm,\n",
    "        processes=[\"signal\", \"ttbar_semilep\"],\n",
    "        max_files_per_process=5,\n",
    "    )\n",
    "    console = Console(force_jupyter=False)\n",
    "    console.print(inspector_rucio.format_dataset_size_table(size_summary))\n",
    "except Exception as exc:\n",
    "    print(\"Skipping Rucio size lookup (set size_summary=None):\", exc)\n",
    "    size_summary = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from intccms.metrics.inspector import (\n",
    "    format_overall_stats_table,\n",
    "    format_branch_stats_table,\n",
    "    format_dataset_stats_table,\n",
    "    format_compression_stats_table,\n",
    ")\n",
    "\n",
    "# Create console\n",
    "console = Console(force_jupyter=False)\n",
    "\n",
    "# Aggregate and display statistics\n",
    "stats = aggregate_statistics(results, size_summary=size_summary)\n",
    "table = format_overall_stats_table(stats)\n",
    "console.print(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Box Plots\n",
    "\n",
    "The box plots in this analysis show statistical distributions:\n",
    "\n",
    "- **Box**: Contains the middle 50% of data (interquartile range, IQR)\n",
    "- **Line inside box**: Median value (50th percentile)\n",
    "- **Whiskers**: Extend to the 5th and 95th percentiles\n",
    "- **Points beyond whiskers**: Outliers outside the 5th-95th percentile range\n",
    "\n",
    "This visualization helps identify data skewness, outliers, and distribution characteristics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Branch Statistics\n",
    "\n",
    "Analyze branch size and compression distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intccms.metrics.inspector.aggregator import compute_branch_statistics\n",
    "\n",
    "branch_stats = compute_branch_statistics(results)\n",
    "table = format_branch_stats_table(branch_stats)\n",
    "console.print(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Per-Dataset Statistics\n",
    "\n",
    "Group results by dataset and compute per-dataset statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by dataset\n",
    "grouped = group_by_dataset(results, dataset_map)\n",
    "\n",
    "# Compute per-dataset statistics\n",
    "dataset_stats = compute_dataset_statistics(grouped, size_summary=size_summary)\n",
    "\n",
    "print(\"\\n=== Per-Dataset Statistics ===\")\n",
    "for dataset_name, ds_stats in dataset_stats.items():\n",
    "    print(f\"\\n{dataset_name}:\")\n",
    "    print(f\"  Files: {ds_stats['num_files']}\")\n",
    "    print(f\"  Total events: {ds_stats['total_events']:,}\")\n",
    "    print(f\"  Avg events/file: {ds_stats['avg_events_per_file']:,.0f}\")\n",
    "    if ds_stats['total_size_bytes'] > 0:\n",
    "        print(f\"  Total size: {ds_stats['total_size_bytes'] / 1024**3:.2f} GB\")\n",
    "        print(f\"  Avg file size: {ds_stats['avg_file_size_bytes'] / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by dataset\n",
    "grouped = group_by_dataset(results, dataset_map)\n",
    "\n",
    "# Compute per-dataset statistics\n",
    "dataset_stats = compute_dataset_statistics(grouped, size_summary=size_summary)\n",
    "\n",
    "# Display as rich table\n",
    "table = format_dataset_stats_table(dataset_stats)\n",
    "console.print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_stats = compute_compression_stats(results)\n",
    "\n",
    "print(\"\\n=== Compression Statistics ===\")\n",
    "print(f\"Files with compression info: {compression_stats['files_with_compression']}\")\n",
    "print(f\"Overall compression ratio: {compression_stats['overall_compression_ratio']:.2f}x\")\n",
    "print(f\"Average tree compression: {compression_stats['avg_tree_compression_ratio']:.2f}x\")\n",
    "print(f\"Median tree compression: {compression_stats['median_tree_compression_ratio']:.2f}x\")\n",
    "print(f\"Total compressed: {compression_stats['total_compressed_bytes'] / 1024**3:.2f} GB\")\n",
    "print(f\"Total uncompressed: {compression_stats['total_uncompressed_bytes'] / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Visualizations\n",
    "\n",
    "Create plots to visualize the inspection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_stats = compute_compression_stats(results)\n",
    "\n",
    "# Display as rich table\n",
    "table = format_compression_stats_table(compression_stats)\n",
    "console.print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Events per file ratio by dataset\n",
    "from intccms.metrics.inspector.plot import plot_events_per_file_by_dataset\n",
    "\n",
    "fig, ax = plot_events_per_file_by_dataset(dataset_stats)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot.plot_event_distribution(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plot.plot_dataset_comparison(dataset_stats)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branch Size Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot.plot_branch_size_distribution(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branch Compression Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot.plot_branch_compression_distribution(results)\n",
    "if fig is not None:\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No compression data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branch Distributions by Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plot.plot_branch_distributions_by_dataset(results, dataset_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Size Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot.plot_file_size_distribution(results, size_summary=size_summary)\n",
    "if fig is not None:\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No file size data available (all files are remote)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Dashboard\n",
    "\n",
    "Create a comprehensive dashboard with all key plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot.plot_summary_dashboard(results, dataset_stats, dataset_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save Results\n",
    "\n",
    "You can save plots to files and export statistics to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save summary dashboard\n",
    "# fig = plot.plot_summary_dashboard(\n",
    "#     results, dataset_stats, dataset_map,\n",
    "#     save_path=\"input_summary.png\"\n",
    "# )\n",
    "\n",
    "# Export statistics to JSON\n",
    "# output = {\n",
    "#     \"overall_stats\": stats,\n",
    "#     \"dataset_stats\": dataset_stats,\n",
    "#     \"compression_stats\": compression_stats,\n",
    "#     \"branch_stats\": branch_stats,\n",
    "# }\n",
    "# \n",
    "# with open(\"inspection_results.json\", \"w\") as f:\n",
    "#     json.dump(output, f, indent=2)\n",
    "\n",
    "print(\"Done! You can save plots and export statistics as needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Dask client and cluster\n",
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}