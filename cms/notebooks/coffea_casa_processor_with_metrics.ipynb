{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coffea-Casa Processor-Based Workflow Test with Metrics\n",
    "\n",
    "This notebook demonstrates the UnifiedProcessor workflow with coffea.processor.Runner on Coffea-Casa, including skimming, analysis, histogramming, statistics steps, and **comprehensive performance metrics collection**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Overview\n",
    "\n",
    "1. Setup Python path for intccms package\n",
    "2. Install dependencies and register modules for cloud pickle\n",
    "3. Acquire Dask client from Coffea-Casa environment\n",
    "4. Configure analysis parameters (including metrics)\n",
    "5. Run metadata extraction\n",
    "6. Initialize UnifiedProcessor\n",
    "7. Run processor with coffea.processor.Runner\n",
    "8. **Collect and display performance metrics**\n",
    "9. Save histograms\n",
    "10. Run statistical analysis (if enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Python path to include intccms package\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to Python path\n",
    "repo_root = Path.cwd()\n",
    "src_dir = repo_root / \"src\"\n",
    "examples_dir = repo_root\n",
    "if str(src_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(src_dir))\n",
    "if str(examples_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(examples_dir))\n",
    "print(f\"‚úÖ Added {src_dir} to Python path\")\n",
    "print(f\"‚úÖ Added {examples_dir} to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COFFEA_VERSION = \"2025.10.3.dev17+g2cde65fb6\" # 2025.10.2\n",
    "COFFEA_PIP = \"git+https://github.com/scikit-hep/coffea@master\"\n",
    "try:\n",
    "    import omegaconf\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è omegaconf not found, installing...\")\n",
    "    ! pip install omegaconf\n",
    "\n",
    "try:\n",
    "    import coffea\n",
    "    print(\"Coffea version: \", coffea.__version__)\n",
    "    # assert coffea.__version__ == \"2025.10.3.dev9+g41c84f7a9\"\n",
    "except (ImportError, AssertionError):\n",
    "    print(\"‚ö†Ô∏è coffea not found or incorrect version, installing...\")\n",
    "    ! pip install $COFFEA_PIP\n",
    "print(\"‚úÖ All dependencies are installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and cloudpickle registration\n",
    "import copy\n",
    "import os\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = \"\"\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = \"\"\n",
    "\n",
    "from dask.distributed import Client, PipInstall\n",
    "from coffea.processor import DaskExecutor\n",
    "from coffea.nanoevents import NanoAODSchema\n",
    "\n",
    "import cloudpickle\n",
    "import intccms\n",
    "import example_cms\n",
    "\n",
    "# Register modules for cloud pickle\n",
    "cloudpickle.register_pickle_by_value(intccms)\n",
    "cloudpickle.register_pickle_by_value(example_cms)\n",
    "\n",
    "from example_cms.configs.configuration import config as original_config\n",
    "from intccms.schema import Config, load_config_with_restricted_cli\n",
    "from intccms.utils.output import OutputDirectoryManager\n",
    "from intccms.metadata_extractor import DatasetMetadataManager\n",
    "from intccms.datasets import DatasetManager\n",
    "from intccms.analysis import run_processor_workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire Dask Client\n",
    "\n",
    "Coffea-Casa provides a shared scheduler. Connect to it and register dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_client():\n",
    "    \"\"\"Acquire Dask client from Coffea-Casa environment.\"\"\"\n",
    "    client = Client(\"tls://localhost:8786\")\n",
    "    dependencies = [COFFEA_PIP] #[\"coffea==2025.10.2\"]\n",
    "    client.register_plugin(PipInstall(packages=dependencies))\n",
    "    cluster = None  # no local cluster in this mode\n",
    "    return client, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Setup\n",
    "\n",
    "Configure analysis parameters including which processes to run, output settings, and **metrics collection**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration setup\n",
    "config = copy.deepcopy(original_config)\n",
    "\n",
    "# Limit files for testing\n",
    "config[\"datasets\"][\"max_files\"] = None\n",
    "\n",
    "# Use local output directory\n",
    "config[\"general\"][\"output_dir\"] = \"example_cms/outputs/\"\n",
    "\n",
    "# Configuration flags\n",
    "config[\"general\"][\"read_from_cache\"] = False\n",
    "config[\"general\"][\"run_metadata_generation\"] = False\n",
    "config[\"general\"][\"run_processor\"] = True  # Set to False to skip processor and load saved histograms\n",
    "config[\"general\"][\"save_skimmed_output\"] = False  # Set to True to save filtered events to disk\n",
    "config[\"general\"][\"run_analysis\"] = True\n",
    "config[\"general\"][\"run_histogramming\"] = True\n",
    "config[\"general\"][\"run_systematics\"] = True\n",
    "config[\"general\"][\"run_statistics\"] = False\n",
    "\n",
    "# ===== ENABLE METRICS COLLECTION =====\n",
    "config[\"general\"][\"metrics\"] = {\n",
    "    \"enable\": True,                    # Master switch\n",
    "    \"track_workers\": True,             # Enable scheduler-based tracking\n",
    "    \"save_measurements\": True,         # Save to disk\n",
    "}\n",
    "\n",
    "# Test only signal dataset\n",
    "#config[\"general\"][\"processes\"] = [\"data\"]\n",
    "\n",
    "cli_args = []\n",
    "full_config = load_config_with_restricted_cli(config, cli_args)\n",
    "validated_config = Config(**full_config)\n",
    "\n",
    "print(f\"‚úÖ Configuration loaded with max_files={validated_config.datasets.max_files}\")\n",
    "print(f\"   - run_processor: {validated_config.general.run_processor}\")\n",
    "print(f\"   - save_skimmed_output: {validated_config.general.save_skimmed_output}\")\n",
    "print(f\"   - run_analysis: {validated_config.general.run_analysis}\")\n",
    "print(f\"   - run_histogramming: {validated_config.general.run_histogramming}\")\n",
    "print(f\"   - run_systematics: {validated_config.general.run_systematics}\")\n",
    "print(f\"   - run_statistics: {validated_config.general.run_statistics}\")\n",
    "print(f\"   - metrics.enable: {validated_config.general.metrics.enable}\")\n",
    "print(f\"   - metrics.track_workers: {validated_config.general.metrics.track_workers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Complete Workflow\n",
    "\n",
    "Execute the full processor workflow with proper cleanup in a try/finally block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    client, cluster = acquire_client()\n",
    "    print(f\"‚úÖ Connected to Dask scheduler\")\n",
    "    print(f\"üìä Dashboard: {client.dashboard_link}\")\n",
    "    \n",
    "    # Output Manager Setup\n",
    "    output_manager = OutputDirectoryManager(\n",
    "        root_output_dir=validated_config.general.output_dir,\n",
    "        cache_dir=validated_config.general.cache_dir,\n",
    "        metadata_dir=validated_config.general.metadata_dir,\n",
    "        skimmed_dir=validated_config.general.skimmed_dir\n",
    "    )\n",
    "    print(f\"‚úÖ Output directory: {output_manager.root_output_dir}\")\n",
    "\n",
    "    # Step 1: Metadata Extraction\n",
    "    print(\"\\nüìã Extracting metadata...\")\n",
    "    dataset_manager = DatasetManager(validated_config.datasets)\n",
    "    metadata_generator = DatasetMetadataManager(\n",
    "        dataset_manager=dataset_manager,\n",
    "        output_manager=output_manager,\n",
    "        executor=DaskExecutor(client=client),\n",
    "        config=validated_config,\n",
    "    )\n",
    "    metadata_generator.run(\n",
    "        generate_metadata=validated_config.general.run_metadata_generation,\n",
    "        processes_filter=validated_config.general.processes if hasattr(validated_config.general, 'processes') else None\n",
    "    )\n",
    "\n",
    "    metadata_lookup = metadata_generator.build_metadata_lookup()\n",
    "    workitems = metadata_generator.workitems\n",
    "\n",
    "    print(f\"‚úÖ Generated {len(workitems)} workitems\")\n",
    "\n",
    "    # Show first few workitems\n",
    "    print(\"\\nüîç Workitem Details (first 5):\")\n",
    "    for i, wi in enumerate(workitems[:5]):\n",
    "        print(f\"  {i}: dataset='{wi.dataset}' process='{wi.usermeta.get('process', 'N/A')}'\")\n",
    "    if len(workitems) > 5:\n",
    "        print(f\"  ... and {len(workitems) - 5} more\")\n",
    "\n",
    "    # Step 2: Run Processor Workflow (or load saved histograms)\n",
    "    print(\"\\nüöÄ Running processor workflow...\")\n",
    "    t0 = time.perf_counter()\n",
    "    output, report, metrics = run_processor_workflow(\n",
    "        config=validated_config,\n",
    "        output_manager=output_manager,\n",
    "        metadata_lookup=metadata_lookup,\n",
    "        workitems=workitems[:],\n",
    "        executor=DaskExecutor(client=client, treereduction=6, retries=0),\n",
    "        schema=NanoAODSchema,\n",
    "    )\n",
    "    t1 = time.perf_counter()\n",
    "    print(\"‚úÖ Processor workflow complete!\")\n",
    "\n",
    "    # Step 3: Display Results\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä Results:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if validated_config.general.run_processor:\n",
    "        print(f\"üìä Total events processed: {output.get('processed_events', 0):,}\")\n",
    "        if 'skimmed_events' in output:\n",
    "            print(f\"‚úÇÔ∏è  Events after skim: {output.get('skimmed_events', 0):,}\")\n",
    "\n",
    "    # Histograms are auto-saved by processor\n",
    "    if output and \"histograms\" in output:\n",
    "        num_histograms = sum(len(hists) for hists in output[\"histograms\"].values())\n",
    "        print(f\"üìà Total histograms: {num_histograms}\")\n",
    "        print(f\"üìà Channels: {list(output['histograms'].keys())}\")\n",
    "        print(f\"‚úÖ Histograms auto-saved to: {output_manager.histograms_dir}\")\n",
    "        print(f\"   - processor_histograms.pkl (for loading with run_processor=False)\")\n",
    "        print(f\"   - histograms.root (for downstream tools)\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No histograms produced (run_histogramming may be disabled)\")\n",
    "\n",
    "    # Step 4: Run Statistical Analysis\n",
    "    if validated_config.general.run_statistics and output and \"histograms\" in output:\n",
    "        print(\"\\nüìä Running statistical analysis...\")\n",
    "\n",
    "        # Create analysis instance for statistics\n",
    "        from intccms.analysis.nondiff import NonDiffAnalysis\n",
    "\n",
    "        analysis = NonDiffAnalysis(validated_config, output_manager)\n",
    "        # Set histograms from processor output\n",
    "        analysis.nD_hists_per_region = output[\"histograms\"]\n",
    "\n",
    "        # Check if cabinetry config exists\n",
    "        if hasattr(validated_config, 'statistics') and hasattr(validated_config.statistics, 'cabinetry_config'):\n",
    "            cabinetry_config_path = validated_config.statistics.cabinetry_config\n",
    "\n",
    "            # Check if file exists\n",
    "            if Path(cabinetry_config_path).exists():\n",
    "                print(f\"‚úÖ Using cabinetry config: {cabinetry_config_path}\")\n",
    "                analysis.run_statistics(cabinetry_config_path)\n",
    "                print(f\"‚úÖ Statistical analysis complete!\")\n",
    "                print(f\"üìä Plots saved to: {output_manager.statistics_dir}\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  Cabinetry config not found: {cabinetry_config_path}\")\n",
    "                print(f\"   Skipping statistics step\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  No cabinetry_config specified in configuration\")\n",
    "            print(f\"   Skipping statistics step\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Statistics step skipped (disabled or no histograms)\")\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ Complete processor workflow finished!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "finally:\n",
    "    # Cleanup\n",
    "    print(\"\\nüßπ Cleaning up...\")\n",
    "    # Note: Don't close client yet - we need it for metrics display\n",
    "    print(\"‚úÖ Workflow complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "\n",
    "Display comprehensive performance metrics collected during processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Coffea Report\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Rich for beautiful table display\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä Processing Metrics\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display metrics if collection was enabled\n",
    "if metrics:\n",
    "    from intccms.metrics import (\n",
    "        format_throughput_table,\n",
    "        format_event_processing_table,\n",
    "        format_resources_table,\n",
    "        format_timing_table,\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìà Throughput Metrics\")\n",
    "    console.print(format_throughput_table(metrics))\n",
    "    \n",
    "    print(\"\\n‚ö° Event Processing Metrics\")\n",
    "    console.print(format_event_processing_table(metrics))\n",
    "    \n",
    "    print(\"\\nüñ•Ô∏è  Resource Utilization\")\n",
    "    console.print(format_resources_table(metrics))\n",
    "    \n",
    "    print(\"\\n‚è±Ô∏è  Timing Breakdown\")\n",
    "    console.print(format_timing_table(metrics))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Metrics collection was disabled (set config.general.metrics.enable=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Calculations for Verification\n",
    "\n",
    "Compare automated metrics to manual calculations from the coffea report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîç Manual Calculations (for verification)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if report:\n",
    "    print(f\"data read: {report['bytesread'] / 1000**3:.2f} GB in {report['chunks']} chunks\")\n",
    "    print(f\"\")\n",
    "    print(f\"core-average event rate using 'processtime': {report['entries'] / 1000 / report['processtime']:.2f} kHz\")\n",
    "    print(f\"core-average data rate using 'processtime': {report['bytesread'] / 1000**3 * 8 / report['processtime']:.2f} Gbps\")\n",
    "    print(f\"\")\n",
    "    print(f\"average event rate using walltime: {report['entries'] / 1000 / (t1 - t0):.2f} kHz\")\n",
    "    print(f\"average data rate using walltime: {report['bytesread'] / 1000**3 * 8 / (t1 - t0):.2f} Gbps\")\n",
    "    print(f\"\")\n",
    "    print(f\"Number of branches read: {len(report['columns'])}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Compare manual calculations to metrics tables above!\")\n",
    "    print(\"   - Wall-clock rates should match 'Event Rate (Wall Clock)' and 'Data Rate'\")\n",
    "    print(\"   - Processtime rates should match 'Event Rate (Aggregated)'\")\n",
    "else:\n",
    "    print(\"No report available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Performance Report\n",
    "\n",
    "Link to the detailed Dask performance report HTML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if metrics and validated_config.general.metrics.track_workers:\n    perf_report_path = output_manager.benchmarks_dir / \"latest\" / \"dask_performance.html\"\n    print(f\"\\nüìä Dask Performance Report: {perf_report_path}\")\n    print(\"   Download this file and open in a browser for detailed task timeline visualization\")\n    print(\"   Includes task execution timeline, worker utilization, and communication patterns\")\nelse:\n    print(\"\\n‚ö†Ô∏è  Performance report not generated (metrics.track_workers=False)\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Performance Visualizations\n\nGenerate plots to visualize worker scaling, memory/CPU utilization, and throughput over time.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Generate visualizations if worker tracking was enabled\nif metrics and validated_config.general.metrics.track_workers:\n    from intccms.metrics import (\n        load_worker_timeline,\n        plot_summary_dashboard,\n        plot_worker_count_timeline,\n        plot_memory_utilization_timeline,\n        plot_cpu_utilization_timeline,\n        plot_scaling_efficiency,\n    )\n    \n    # Find the latest benchmark directory\n    benchmarks_dir = output_manager.benchmarks_dir\n    latest_dirs = sorted(benchmarks_dir.glob(\"*\"), key=lambda p: p.name, reverse=True)\n    \n    if latest_dirs:\n        measurement_path = latest_dirs[0]  # Most recent timestamped directory\n        print(f\"üìä Loading worker tracking data from: {measurement_path}\")\n        \n        try:\n            # Load tracking data\n            tracking_data = load_worker_timeline(measurement_path)\n            \n            print(f\"   - Tracking duration: {len(tracking_data['worker_counts'])} samples\")\n            print(f\"   - Workers tracked: {len(tracking_data['worker_memory'])}\")\n            \n            # Generate summary dashboard\n            print(\"\\nüìä Generating summary dashboard...\")\n            fig = plot_summary_dashboard(\n                tracking_data, \n                metrics, \n                output_path=measurement_path / \"summary_dashboard.png\"\n            )\n            \n            print(f\"‚úÖ Dashboard saved to: {measurement_path / 'summary_dashboard.png'}\")\n            \n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Failed to generate visualizations: {e}\")\n    else:\n        print(\"‚ö†Ô∏è  No benchmark directories found\")\nelse:\n    print(\"‚ö†Ô∏è  Worker tracking was disabled - no visualizations to generate\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Optional: Generate individual plots for detailed analysis\n# Uncomment the plots you want to generate\n\nif metrics and validated_config.general.metrics.track_workers and latest_dirs:\n    measurement_path = latest_dirs[0]\n    \n    try:\n        tracking_data = load_worker_timeline(measurement_path)\n        \n        # Worker count timeline\n        # fig, ax = plot_worker_count_timeline(\n        #     tracking_data, \n        #     output_path=measurement_path / \"worker_count.png\"\n        # )\n        \n        # Memory utilization timeline\n        # fig, ax = plot_memory_utilization_timeline(\n        #     tracking_data,\n        #     output_path=measurement_path / \"memory_utilization.png\"\n        # )\n        \n        # CPU utilization timeline\n        # fig, ax = plot_cpu_utilization_timeline(\n        #     tracking_data,\n        #     output_path=measurement_path / \"cpu_utilization.png\"\n        # )\n        \n        # Scaling efficiency\n        # fig, ax = plot_scaling_efficiency(\n        #     tracking_data,\n        #     metrics,\n        #     output_path=measurement_path / \"scaling_efficiency.png\"\n        # )\n        \n        print(\"üí° Tip: Uncomment the plots above to generate individual visualizations\")\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è  Error: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Close the Dask client connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close client\n",
    "client.close()\n",
    "print(\"‚úÖ Dask client closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}