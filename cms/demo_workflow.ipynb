{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082a7547",
   "metadata": {},
   "source": [
    "# IRIS-HEP CMS Integration Challenge: Workflow Demonstration\n",
    "\n",
    "This notebook demonstrates the workflow of the IRIS-HEP CMS integration challenge. The workflow is modular and consists of the following steps:\n",
    "\n",
    "1. Environment Setup\n",
    "2. Configuration Construction\n",
    "3. Metadata Extraction\n",
    "4. Skimming\n",
    "5. Histogramming\n",
    "6. Statistical Analysis\n",
    "7. Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c51884",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "To set up the environment, use the following command to launch JupyterLab with the required dependencies:\n",
    "\n",
    "```sh\n",
    "pixi run lab\n",
    "```\n",
    "\n",
    "Alternatively, if you prefer a Conda-like environment, activate it using the provided script:\n",
    "\n",
    "```sh\n",
    "source pixi_activate.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1e767-0be5-49d6-b316-aff8a7df67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [coffea-casa] Install packages\n",
    "!pip install omegaconf\n",
    "#!pip install coffea==2025.10.0\n",
    "!pip install s3pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c5d9d-f273-4b38-b0ab-0a944e10d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask stuff\n",
    "# Register some modules for dask workers to pickle/unpickle\n",
    "import cloudpickle\n",
    "import utils, user\n",
    "cloudpickle.register_pickle_by_value(utils)\n",
    "cloudpickle.register_pickle_by_value(user)\n",
    "\n",
    "# Setup some funcitons for dask client handling\n",
    "from dask.distributed import Client, LocalCluster, PipInstall\n",
    "\n",
    "COFFEA_CASA = True\n",
    "NEW_BUCKET = True\n",
    "\n",
    "from dask.distributed import WorkerPlugin\n",
    "import os\n",
    "\n",
    "class EnvSetter(WorkerPlugin):\n",
    "    def __init__(self, env_vars):\n",
    "        self.env_vars = env_vars\n",
    "\n",
    "    def setup(self, worker):\n",
    "        import os\n",
    "        os.environ.update(self.env_vars)\n",
    "        print(f\"Set env on worker {worker.address}: {self.env_vars}\")\n",
    "\n",
    "ID = \"M4S6FZBLCE2MT5R5R3TE\" if NEW_BUCKET else os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "KEY = \"1POhQ20OjZWViVuIan0bqVuegnYR8LUKmGFbpB7T\" if NEW_BUCKET else os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "\n",
    "env_setter_plugin = EnvSetter({\"AWS_ACCESS_KEY_ID\": ID, \"AWS_SECRET_ACCESS_KEY\": KEY})\n",
    "\n",
    "def get_client(n_workers=4):\n",
    "    if not COFFEA_CASA:\n",
    "        cluster = LocalCluster(n_workers=n_workers, processes=True, threads_per_worker=1)\n",
    "        client = Client(cluster)\n",
    "        print(f\"Dask dashboard: {cluster.dashboard_link}\")\n",
    "    else:\n",
    "        dependencies = [\"git+https://github.com/MoAly98/coffea.git@feat/storage_opts_parquet\", \"s3pathlib\"]\n",
    "        client = Client(\"tls://localhost:8786\")\n",
    "        client.register_plugin(PipInstall(packages=dependencies))\n",
    "        client.register_plugin(env_setter_plugin)\n",
    "        cluster = None  # no local cluster in this mode\n",
    "    return client, cluster\n",
    "\n",
    "def close_client(cluster, client):\n",
    "    if client:\n",
    "        client.close()\n",
    "    if cluster:\n",
    "        cluster.close()\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fc6967",
   "metadata": {},
   "source": [
    "## 2. Configuration Construction\n",
    "\n",
    "A lightweight Python config defines general settings, datasets, cuts, observables, and systematics — all are type-checked and (mostly) CLI-overrideable.\n",
    "Everything else in the workflow reads from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ada530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Rich-based Configuration Display from logging module\n",
    "from utils.logging import display_config_table, get_config_logger\n",
    "\n",
    "# Create a global config logger instance for this notebook\n",
    "config_logger = get_config_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0534706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Demonstrate configuration comparison and change detection\n",
    "from user.configuration import config as original_config\n",
    "from utils.schema import Config, load_config_with_restricted_cli\n",
    "\n",
    "import copy\n",
    "\n",
    "# Save the original configuration for comparison\n",
    "config = copy.deepcopy(original_config)\n",
    "\n",
    "print(\"=== Full configuration ===\")\n",
    "display_config_table(config, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b2f6ae-0fc6-44d4-80ed-1e03ee9c51cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the datasets config\n",
    "display_config_table({\"datasets\": config[\"datasets\"]},\n",
    "                    expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30b2785-4e3e-46dd-a9fa-158fa7dd32ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a modification\n",
    "config[\"datasets\"][\"max_files\"] = 1 # Limit to 1 files per dataset\n",
    "config[\"general\"][\"processes\"] = [\"signal\", \"ttbar_semilep\"]\n",
    "\n",
    "print(\"=== SHOW ONLY CHANGES ===\")\n",
    "display_config_table({\"datasets\": config[\"datasets\"]},\n",
    "                    expand=True,\n",
    "                    compare_with={\"datasets\": original_config[\"datasets\"]},\n",
    "                    show_only_changes=True)\n",
    "\n",
    "cli_args = []  # No CLI overrides for this demo\n",
    "full_config = load_config_with_restricted_cli(config, cli_args)\n",
    "\n",
    "print(\"✅ Processed CLI arguments and loaded full configuration\")\n",
    "validated_config = Config(**full_config)  # This is the key validation step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29092ea4",
   "metadata": {},
   "source": [
    "# 3. Output Manager & Directory Structure\n",
    "\n",
    "The framework uses a centralized `OutputDirectoryManager` (`utils/output_manager.py`) to organize all analysis outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c26af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.output_manager import OutputDirectoryManager\n",
    "\n",
    "output_manager = OutputDirectoryManager(\n",
    "    root_output_dir=validated_config.general.output_dir,\n",
    "    cache_dir=validated_config.general.cache_dir,\n",
    "    metadata_dir=validated_config.general.metadata_dir,\n",
    "    skimmed_dir=validated_config.general.skimmed_dir\n",
    ")\n",
    "\n",
    "print(\"✅ Created OutputDirectoryManager with validated paths\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa7bd4",
   "metadata": {},
   "source": [
    "## 4. Metadata Extraction\n",
    "\n",
    "Metadata extraction runs `coffea`'s preprocessor to build analysis workitems, a `coffea`-compatible fileset and a summary of the processed `NanoAOD`s. Its outputs stay in memory, but are also stored in `JSON` files to allow re-runing subsequent steps without pre-processing every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb1e349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metadata_extractor import NanoAODMetadataGenerator\n",
    "from utils.datasets import ConfigurableDatasetManager\n",
    "\n",
    "# Step 1: Create dataset manager instance\n",
    "dataset_manager = ConfigurableDatasetManager(validated_config.datasets)\n",
    "\n",
    "# Step 2: Create metadata generator instance\n",
    "validated_config.general.run_metadata_generation=False\n",
    "\n",
    "client, cluster = None, None #get_client()\n",
    "metadata_generator = NanoAODMetadataGenerator(\n",
    "    dataset_manager=dataset_manager,\n",
    "    output_manager=output_manager\n",
    ")\n",
    "# In the back this uses coffea's preprocess function\n",
    "metadata = metadata_generator.run(\n",
    "    generate_metadata=validated_config.general.run_metadata_generation,\n",
    "    processes_filter=validated_config.general.processes if hasattr(validated_config.general, 'processes') else None\n",
    ")\n",
    "#close_client(client, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ecc72",
   "metadata": {},
   "source": [
    "# 5. Skimming & Event Selection\n",
    "\n",
    "The skimming step processes the workitems from previous step, and applies event and branch selections with `dask` and a `coffea`-like processor (see [Alex's issue](https://github.com/scikit-hep/coffea/issues/1393)). Currently the skimming results are in-memory events, but also stored on disk as `ROOT` files to avoid having to run this multiple times. The events are also cached in `.pkl` files for faster re-reading. Eventually this step needs to be integrated with subsequent steps in a complete `coffea` or `coffea`-like processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442e7495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.skimming import process_workitems_with_skimming\n",
    "\n",
    "# Extract workitems and fileset from metadata generator\n",
    "fileset = metadata_generator.fileset\n",
    "workitems = metadata_generator.workitems\n",
    "\n",
    "print(f\"📊 Processing {len(workitems)} workitems across {len(fileset)} datasets\")\n",
    "\n",
    "# Disable caching for demonstration\n",
    "validated_config.general.read_from_cache = False\n",
    "validated_config.general.run_skimming = True\n",
    "\n",
    "# Skim data with dask according to the workitems\n",
    "datasets = process_workitems_with_skimming(\n",
    "    workitems,\n",
    "    validated_config,\n",
    "    output_manager,\n",
    "    datasets,\n",
    "    metadata_generator.nanoaods_summary\n",
    ")\n",
    "\n",
    "# Display the structure of processed datasets\n",
    "print(f\"\\n✅ Skimming complete! Processed datasets structure:\")\n",
    "for dataset in datasets:\n",
    "    if dataset.events:\n",
    "        print(f\"  📁 {dataset.name} processed\")\n",
    "        for i, (events, metadata) in enumerate(dataset.events):\n",
    "            print(f\"    └── File {i+1}: {len(events)} events, {len(events.fields)} branches\")\n",
    "\n",
    "print(f\"\\n💾 Skimmed data saved to: {output_manager.get_skimmed_dir()}\")\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(dataset.events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b7f222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable caching to show how it speeds up subsequent runs\n",
    "validated_config.general.read_from_cache = True\n",
    "validated_config.general.run_skimming = False\n",
    "# Run the skimming again - it will look for cached files\n",
    "# if they don't exist, it will fallback to skim regularly\n",
    "cached_datasets = process_workitems_with_skimming(\n",
    "    workitems,\n",
    "    validated_config,\n",
    "    output_manager,\n",
    "    fileset,\n",
    "    metadata_generator.nanoaods_summary\n",
    ")\n",
    "\n",
    "# Show cache directory contents\n",
    "import os\n",
    "cache_files = os.listdir(output_manager.get_skimmed_dir())\n",
    "print(f\"💾 Cached files in skimmed directory: {len(cache_files)} files\")\n",
    "print(f\"📁 Cache location: {output_manager.get_cache_dir()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c3086d",
   "metadata": {},
   "source": [
    "# 6. Analysis & Histogramming\n",
    "\n",
    "This step encapsulates a few underlying function calls:\n",
    "1. Global event selection is applied\n",
    "2. If MVA training is configured and enabled, an MVA training will be triggered\n",
    "3. Apply corrections from `correctionlib`\n",
    "4. Ghost observables will be computed and added to event record\n",
    "6. Channels will be built with their corresponding selections\n",
    "7. Compute all observables (once nomina, once per systematic variation)\n",
    "8. Create histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d78ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the analysis class for the analysis\n",
    "from analysis.nondiff import NonDiffAnalysis\n",
    "from utils.output_files import save_histograms_to_root\n",
    "\n",
    "validated_config.general.run_histogramming=True\n",
    "\n",
    "# Initialize the analysis object\n",
    "# This analysis object will handle histogram creation, observable computation, and systematic variations\n",
    "nondiff_analysis = NonDiffAnalysis(validated_config, datasets, output_manager)\n",
    "\n",
    "print(f\"🔬 Analysis initialized for {len(nondiff_analysis.datasets)} datasets\")\n",
    "\n",
    "# Loop over each dataset and its associated event data\n",
    "histogram_count = 0\n",
    "for dataset in datasets:\n",
    "    if dataset.events:\n",
    "        for events, metadata in dataset.events:\n",
    "            print(f\"   • Processing {len(events)} events with metadata: {metadata['process']}\")\n",
    "            nondiff_analysis.process(events, metadata)\n",
    "            histogram_count += 1\n",
    "\n",
    "save_histograms_to_root(\n",
    "    nondiff_analysis.nD_hists_per_region,\n",
    "    output_file=nondiff_analysis.output_manager.get_histograms_dir() / \"histograms.root\",\n",
    ")\n",
    "\n",
    "print(f\"📈 Generated histograms for channels: {[ch.name for ch in validated_config.channels]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64a7776",
   "metadata": {},
   "source": [
    "# 7. Statistical Analysis\n",
    "\n",
    "The statistical analysis step performs a fit from the histograms and visualize results. This uses `cabinetry` with a manually created config. It also creates and stores the corresponding `pyhf` workspace.\n",
    "\n",
    "**Note: the following fit is performed with the full CMS open-data dataset, not the small example here** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac3572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cabinetry\n",
    "validated_config.statistics.cabinetry_config = \"./example-old/outputs/cabinetry/\"\n",
    "cabinetry_config = cabinetry.configuration.load(\n",
    "    validated_config.statistics.cabinetry_config\n",
    ")\n",
    "data, fit_results, pre_fit_predictions, postfit_predictions = (\n",
    "    nondiff_analysis.run_fit(cabinetry_config=cabinetry_config)\n",
    ")\n",
    "cabinetry.visualize.data_mc(\n",
    "    pre_fit_predictions,\n",
    "    data,\n",
    "    close_figure=False,\n",
    "    config=cabinetry_config,\n",
    "    figure_folder=nondiff_analysis.output_manager.get_statistics_dir(),\n",
    "\n",
    ")\n",
    "cabinetry.visualize.data_mc(\n",
    "    postfit_predictions,\n",
    "    data,\n",
    "    close_figure=False,\n",
    "    config=cabinetry_config,\n",
    "    figure_folder=nondiff_analysis.output_manager.get_statistics_dir(),\n",
    ")\n",
    "cabinetry.visualize.pulls(fit_results, close_figure=False, figure_folder=nondiff_analysis.output_manager.get_statistics_dir(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1455dcd8",
   "metadata": {},
   "source": [
    "## 8. Full Workflow\n",
    "\n",
    "The above workflow is broken down for demonstration, but the simplest way to use it is by putting the main pieces together in one simple steering script and running it from the command-line with CLI overrides of config options. The current implementation is steered by `analysis.py` and can be simply executed with:\n",
    "\n",
    "```sh\n",
    "python analysis.py\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
