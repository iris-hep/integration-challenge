{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMS search for a Z' boson in the single-lepton channel with full Run 2 dataset\n",
    "\n",
    "This notebook demonstrates the Z' → tt̄ single-lepton analysis workflow on various AFs, including skimming, analysis, histogramming, and statistics steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Overview\n",
    "\n",
    "1. Setup Python path for intccms package\n",
    "2. Install dependencies and register modules for cloud pickle\n",
    "3. Acquire Dask client from AF environment\n",
    "4. Configure analysis parameters\n",
    "5. Run metadata extraction (`coffea` preprocessing)\n",
    "6. Initialize analysis processor\n",
    "7. Run processor with coffea.processor.Runner\n",
    "8. Save histograms and run statistical analysis (if enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AF flag\n",
    "We might want to run this code on different facilities, which may each have their own limitations or require different dask client setups. To make it easy to switch between facilities, just set the `AF` variable to the one of your choice. If your `AF` does not exist yet, you can introduce it in this notebook in the relevant sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AF=\"coffeacasa-condor\" # options currently supported: [coffeacasa-condor, coffeacasa-gateway, purdue-af]\n",
    "AUTO_CLOSE_CLIENT=False # the client setup is done with a contextmanager -- this flag decides if we automatically close the client as we exit the manager. If False, you handle closing manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The intccms package\n",
    "The CMS implementation of the integration challenge is set in a package-like structure, which means we hae to add the source code to the python path. The package is referred to as `intccms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Python path to include intccms package\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to Python path\n",
    "repo_root = Path.cwd()\n",
    "src_dir = repo_root / \"src\"\n",
    "examples_dir = repo_root\n",
    "if str(src_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(src_dir))\n",
    "if str(examples_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(examples_dir))\n",
    "print(f\"✅ Added {src_dir} to Python path\")\n",
    "print(f\"✅ Added {examples_dir} to Python path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing extra dependencies\n",
    "The `intccms` package requires `omegaconf` and `roastcoffea`, which is not by default on an AF. `roastcoffea` is a tool developed while working on this project and it provides an API to extract metrics from coffea-processor workflows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import omegaconf\n",
    "except ImportError:\n",
    "    print(\"⚠️ omegaconf not found, installing...\")\n",
    "    ! pip install omegaconf;\n",
    "\n",
    "try:\n",
    "    import roastcoffea\n",
    "except ImportError:\n",
    "    print(\"⚠️ roastcoffea not found, installing...\")\n",
    "    ! pip install roastcoffea;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative coffea version\n",
    "In some cases, we might need to install our own `coffea` version which is not on the AF. For example, when testing a new feature or using a recently realased version with a fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COFFEA_VERSION = \"2025.12.0\"\n",
    "COFFEA_PIP = f\"coffea=={COFFEA_VERSION}\" if \"git\" not in COFFEA_VERSION else COFFEA_VERSION\n",
    "\n",
    "! pip install $COFFEA_PIP ;\n",
    "\n",
    "# Pip-installable dependencies to install on workers\n",
    "WORKER_DEPENDENCIES = [COFFEA_PIP, \"roastcoffea==0.1.2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports from stdlib and other libraries\n",
    "\n",
    "In this notebook we use `dask` and `coffea`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "import cloudpickle\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "\n",
    "from coffea.processor import DaskExecutor, IterativeExecutor\n",
    "from coffea.nanoevents import NanoAODSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports from intccms and other integration-challenge specific tooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intccms\n",
    "from intccms.schema import Config, load_config_with_restricted_cli\n",
    "from intccms.utils.output import OutputDirectoryManager\n",
    "from intccms.metadata_extractor import DatasetMetadataManager\n",
    "from intccms.datasets import DatasetManager\n",
    "from intccms.analysis import run_processor_workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering packages with cloudpickle\n",
    "The intccms cannot be installed on the workers via `pip`, and the configuration files are in python modules which also cannot be installed on the workers. So we need to register them with `cloudpickle` to allow dask to serialize them and send them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import intccms\n",
    "import example_cms\n",
    "\n",
    "# Register modules for cloud pickle\n",
    "cloudpickle.register_pickle_by_value(intccms)\n",
    "cloudpickle.register_pickle_by_value(example_cms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask client setup\n",
    "\n",
    "This notebook uses the `DaskExecutor` from `coffea` to distribute the task graph on the AF. The client setup varies in different facilities, so we implement a function which returns the correct client. The function does so by providing a context manager, within which the client is alive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intccms.utils.dask_client import acquire_client, live_prints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Setup\n",
    "\n",
    "The CMS analysis implementation is configurable via python modules, which we have to import. For this notebook, the configuration files are found in `example_cms/configs/`. You can modify the modules in this directory manually, or you can dynamically change settings using python dictionary manipulation. Below are some settings of interest that you might want to tune when you are testing your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intccms configuration import\n",
    "from example_cms.configs.configuration import config as original_config\n",
    "\n",
    "# Create a deepcopy that we can manipulate\n",
    "config = copy.deepcopy(original_config)\n",
    "\n",
    "# Limit files for testing\n",
    "config[\"datasets\"][\"max_files\"] = 5 #None # None would run over all availale files\n",
    "\n",
    "# Use local output directory\n",
    "config[\"general\"][\"output_dir\"] = \"example_cms/outputs/\"\n",
    "\n",
    "# Preprocessing (coffea) can be executed once and results loaded\n",
    "config[\"general\"][\"run_metadata_generation\"] = False # If True, run analysis pre-processing\n",
    "\n",
    "# Processer = Skimming (filter and save) + Analysis\n",
    "config[\"general\"][\"run_processor\"] = True  # If True, the coffea processor is executed\n",
    "config[\"general\"][\"run_analysis\"] = True # If True, the analysis part of the processor is executed\n",
    "config[\"general\"][\"save_skimmed_output\"] = False  # If True, skimmed events are saved to disk, otherwise filter executed on-the-fly\n",
    "\n",
    "# Analysis = Systematics + histogramming + statsitics\n",
    "config[\"general\"][\"run_histogramming\"] = True\n",
    "config[\"general\"][\"run_systematics\"] = True\n",
    "config[\"general\"][\"run_statistics\"] = False\n",
    "\n",
    "# Datasets to process, by default this is all datasets\n",
    "#config[\"general\"][\"processes\"] = [\"data\"] \n",
    "\n",
    "cli_args = [] # the code can be ran from CLI, but we don't care here\n",
    "full_config = load_config_with_restricted_cli(config, cli_args)\n",
    "\n",
    "# Validated config gives us a dictionary object with all settings checked to be safe with pydantic\n",
    "validated_config = Config(**full_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Workflow\n",
    "\n",
    "Running the CMS integration challenge workflow is split into a few steps, with a modular design that allows us flexibility. The steps are:\n",
    "\n",
    "1. Setting up output directories\n",
    "2. Building an input dataset manager\n",
    "3. Running or loading the coffea preprocessing\n",
    "4. Run the coffea processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output manager setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_manager = OutputDirectoryManager(\n",
    "    root_output_dir=validated_config.general.output_dir,\n",
    "    cache_dir=validated_config.general.cache_dir,\n",
    "    metadata_dir=validated_config.general.metadata_dir,\n",
    "    skimmed_dir=validated_config.general.skimmed_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Data Redirector (Optional)\n",
    "\n",
    "Override the redirector in the config for accessing dataset files. Useful for testing different storage backends. You can also change this in `example_cms/configs/skim.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override redirector for all datasets\n",
    "# Examples:\n",
    "#   \"root://xcache/\"                    \n",
    "#   \"root://cmsxrootd.fnal.gov/\"\n",
    "#   \"root://cms-xrd-global.cern.ch/\"\n",
    "REDIRECTOR = \"root://xcache/\"  # Change this to use a different redirector\n",
    "\n",
    "print(f\"Initial redirector  {validated_config.datasets.datasets[0].name}: {validated_config.datasets.datasets[0].redirector}\")\n",
    "\n",
    "# Apply to all datasets in config\n",
    "for dataset in validated_config.datasets.datasets:\n",
    " dataset.redirector = REDIRECTOR\n",
    "\n",
    "print(f\"Redirector set to: {REDIRECTOR}\")\n",
    "\n",
    "# Verify the change\n",
    "print(f\"New redirector:  {validated_config.datasets.datasets[0].name}: {validated_config.datasets.datasets[0].redirector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input dataset manager setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_manager = DatasetManager(validated_config.datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coffea preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_generator = DatasetMetadataManager(\n",
    "  dataset_manager=dataset_manager,\n",
    "  output_manager=output_manager,\n",
    "  config=validated_config,\n",
    ")\n",
    "\n",
    "if metadata_generator.generate_metadata:\n",
    "  with acquire_client(AF, close_after=AUTO_CLOSE_CLIENT, pip_packages=WORKER_DEPENDENCIES) as (client, cluster):\n",
    "      metadata_generator.run(executor=DaskExecutor(client=client))\n",
    "else:\n",
    "  metadata_generator.run()  # No client needed\n",
    "\n",
    "# Build metadata lookup and extract workitems\n",
    "metadata_lookup = metadata_generator.build_metadata_lookup()\n",
    "workitems = metadata_generator.workitems\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Analysis Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run processor workflow\n",
    "with acquire_client(AF, close_after=AUTO_CLOSE_CLIENT, pip_packages=WORKER_DEPENDENCIES) as (client, cluster):\n",
    "    t0 = time.perf_counter()\n",
    "    stop = live_prints(client)\n",
    "    output, report = run_processor_workflow(\n",
    "        config=validated_config,\n",
    "        output_manager=output_manager,\n",
    "        metadata_lookup=metadata_lookup,\n",
    "        workitems=workitems,\n",
    "        executor=DaskExecutor(client=client, treereduction=8, retries=0),\n",
    "        schema=NanoAODSchema,\n",
    "    )\n",
    "    stop.set()\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "\n",
    "print(f\"Processor workflow complete in {t1-t0:.1f} seconds!\")\n",
    "\n",
    "# Print summary\n",
    "print(f\"Total events processed: {output.get('processed_events', 0):,}\")\n",
    "print(f\"Events after skim: {output.get('skimmed_events', 0):,}\")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systematic variation diagnostics\n",
    "\n",
    "Grid of ratio-to-nominal plots for each MC process across all systematic variations. Useful for verifying that year-decorrelated systematics have the correct nominal fills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from intccms.utils.output import load_histograms_from_pickle\n",
    "\n",
    "histograms_pkl = output_manager.histograms_dir / \"processor_histograms.pkl\"\n",
    "\n",
    "if not histograms_pkl.exists():\n",
    "    print(f\"No saved histograms at {histograms_pkl} -- run the processor first.\")\n",
    "else:\n",
    "    histograms = load_histograms_from_pickle(histograms_pkl)\n",
    "\n",
    "    # Known year keys for grouping decorrelated variations\n",
    "    corrections_cfg = validated_config.corrections\n",
    "    years = sorted(corrections_cfg.keys(), key=len, reverse=True) if isinstance(corrections_cfg, dict) else []\n",
    "\n",
    "    def get_base_name(source_name):\n",
    "        \"\"\"Strip year suffix to find the base variation name.\"\"\"\n",
    "        for year in years:\n",
    "            if source_name.endswith(f\"_{year}\"):\n",
    "                return source_name[: -len(f\"_{year}\")]\n",
    "        return source_name\n",
    "\n",
    "    for channel in validated_config.channels:\n",
    "        channel_name = channel.name\n",
    "        obs_name = channel.fit_observable\n",
    "        if channel_name not in histograms or obs_name not in histograms[channel_name]:\n",
    "            continue\n",
    "\n",
    "        h = histograms[channel_name][obs_name]\n",
    "        processes = sorted(p for p in h.axes[\"process\"] if p != \"data\")\n",
    "        all_variations = [v for v in h.axes[\"variation\"] if v != \"nominal\"]\n",
    "\n",
    "        if not processes or not all_variations:\n",
    "            continue\n",
    "\n",
    "        # all variations is up, down for each systematic, so len()/2 gives true number of systematics implemented\n",
    "        print(f\"Channel: {channel_name} | Processes: {len(processes)} | Systematic variations: {len(all_variations)/2}\")\n",
    "\n",
    "        # Group variations by base name (year-decorrelated share a row)\n",
    "        groups = {}\n",
    "        for var in all_variations:\n",
    "            if var.endswith(\"_up\"):\n",
    "                source = var[:-3]\n",
    "            elif var.endswith(\"_down\"):\n",
    "                source = var[:-5]\n",
    "            else:\n",
    "                source = var\n",
    "            base = get_base_name(source)\n",
    "            groups.setdefault(base, set()).add(var)\n",
    "\n",
    "        group_names = sorted(groups)\n",
    "        nrows = len(group_names)\n",
    "        ncols = len(processes)\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows, ncols,\n",
    "            figsize=(5 * ncols, 2.5 * nrows),\n",
    "            squeeze=False,\n",
    "        )\n",
    "\n",
    "        for row, base_name in enumerate(group_names):\n",
    "            var_names = sorted(groups[base_name])\n",
    "\n",
    "            # Collect all ratios across processes for this row to set shared y-limits\n",
    "            row_ratios = []\n",
    "            for col, proc in enumerate(processes):\n",
    "                ax = axes[row][col]\n",
    "                nom_vals = h[{\"process\": proc, \"variation\": \"nominal\"}].values(flow=False)\n",
    "                bin_centers = h.axes[\"observable\"].centers\n",
    "\n",
    "                for var in var_names:\n",
    "                    var_vals = h[{\"process\": proc, \"variation\": var}].values(flow=False)\n",
    "                    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "                        ratio = np.where(nom_vals > 0, var_vals / nom_vals, 1.0)\n",
    "                    ax.step(bin_centers, ratio, where=\"mid\", label=var, linewidth=0.8)\n",
    "                    row_ratios.append(ratio)\n",
    "\n",
    "                ax.axhline(1.0, color=\"black\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "                if row == 0:\n",
    "                    ax.set_title(proc, fontsize=10)\n",
    "                if row == nrows - 1:\n",
    "                    ax.set_xlabel(h.axes[\"observable\"].label)\n",
    "                else:\n",
    "                    ax.set_xticklabels([])\n",
    "                if col == 0:\n",
    "                    ax.set_ylabel(f\"{base_name}\\nvar / nom\", fontsize=8)\n",
    "\n",
    "                ax.legend(fontsize=5, ncol=2, loc=\"upper right\")\n",
    "\n",
    "            # Symmetric y-limits from the max deviation across the whole row\n",
    "            all_ratios = np.concatenate(row_ratios)\n",
    "            max_dev = max(np.nanmax(np.abs(all_ratios - 1.0)), 0.01)\n",
    "            margin = max_dev * 1.2\n",
    "            for col in range(ncols):\n",
    "                axes[row][col].set_ylim(1.0 - margin, 1.0 + margin)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
