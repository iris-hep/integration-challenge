{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMS search for a Z' boson in the single-lepton channel with full Run 2 dataset\n",
    "\n",
    "This notebook demonstrates the Z' ‚Üí ttÃÑ single-lepton analysis workflow on various AFs, including skimming, analysis, histogramming, and statistics steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Overview\n",
    "\n",
    "1. Setup Python path for intccms package\n",
    "2. Install dependencies and register modules for cloud pickle\n",
    "3. Acquire Dask client from AF environment\n",
    "4. Configure analysis parameters\n",
    "5. Run metadata extraction (`coffea` preprocessing)\n",
    "6. Initialize analysis processor\n",
    "7. Run processor with coffea.processor.Runner\n",
    "8. Save histograms and run statistical analysis (if enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AF flag\n",
    "We might want to run this code on different facilities, which may each have their own limitations or require different dask client setups. To make it easy to switch between facilities, just set the `AF` variable to the one of your choice. If your `AF` does not exist yet, you can introduce it in this notebook in the relevant sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AF=\"coffeacasa-gateway\" # options currently supported: [coffeacasa-condor, coffeacasa-gateway]\n",
    "AUTO_CLOSE_CLIENT=False # the client setup is done with a contextmanager -- this flag decides if we automatically close the client as we exit the manager. If False, you handle closing manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The intccms package\n",
    "The CMS implementation of the integration challenge is set in a package-like structure, which means we hae to add the source code to the python path. The package is referred to as `intccms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Python path to include intccms package\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to Python path\n",
    "repo_root = Path.cwd()\n",
    "src_dir = repo_root / \"src\"\n",
    "examples_dir = repo_root\n",
    "if str(src_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(src_dir))\n",
    "if str(examples_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(examples_dir))\n",
    "print(f\"‚úÖ Added {src_dir} to Python path\")\n",
    "print(f\"‚úÖ Added {examples_dir} to Python path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installig extra dependencies\n",
    "The `intccms` package requires `omegaconf` and `roastcoffea`, which is not by default on an AF. `roastcoffea` is a tool developed while working on this project and it provides an API to extract metrics from coffea-processor workflows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import omegaconf\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è omegaconf not found, installing...\")\n",
    "    ! pip install omegaconf;\n",
    "\n",
    "try:\n",
    "    import roastcoffea\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è roastcoffea not found, installing...\")\n",
    "    ! pip install roastcoffea;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative coffea version\n",
    "In some cases, we might need to install our own `coffea` version which is not on the AF. For example, when testing a new feature or using a recently realased version with a fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COFFEA_VERSION = \"2025.12.0\"\n",
    "COFFEA_PIP = f\"coffea=={COFFEA_VERSION}\" if \"git\" not in COFFEA_VERSION else COFFEA_VERSION\n",
    "\n",
    "if AF == \"coffeacasa-gateway\" and \"git\" in COFFEA_VERSION:\n",
    "    raise ValueError(\"The coffea-casa dask gateway facility does not support installing packages with pip via https on workers.\")\n",
    "\n",
    "! pip install $COFFEA_PIP ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports from stdlib and other libraries\n",
    "\n",
    "In this notebook we use `dask` and `coffea`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "import cloudpickle\n",
    "from contextlib import contextmanager\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "\n",
    "from coffea.processor import DaskExecutor, IterativeExecutor\n",
    "from coffea.nanoevents import NanoAODSchema\n",
    "from dask.distributed import Client, PipInstall, WorkerPlugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports from intccms and other integration-challenge specific tooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intccms\n",
    "from intccms.schema import Config, load_config_with_restricted_cli\n",
    "from intccms.utils.output import OutputDirectoryManager\n",
    "from intccms.metadata_extractor import DatasetMetadataManager\n",
    "from intccms.datasets import DatasetManager\n",
    "from intccms.analysis import run_processor_workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering packages with cloudpickle\n",
    "The intccms cannot be installed on the workers via `pip`, and the configuration files are in python modules which also cannot be installed on the workers. So we need to register them with `cloudpickle` to allow dask to serialize them and send them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import intccms\n",
    "import example_cms\n",
    "\n",
    "# Register modules for cloud pickle\n",
    "cloudpickle.register_pickle_by_value(intccms)\n",
    "cloudpickle.register_pickle_by_value(example_cms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask client setup\n",
    "\n",
    "This notebook uses the `DaskExecutor` from `coffea` to distribute the task graph on the AF. The client setup varies in different facilities, so we implement a function which returns the correct client. The function does so by providing a context manager, within which the client is alive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def acquire_client(af=AF, close_after=AUTO_CLOSE_CLIENT):\n",
    "    \"\"\"Context manager to acquire and safely close a Dask client from a Coffea-Casa environment.\"\"\"\n",
    "\n",
    "    # These are the pip-installable dependncies which should be installed on workers\n",
    "    dependencies = [COFFEA_PIP, \"roastcoffea==0.1.2\"]\n",
    "    client, cluster = None, None\n",
    "    \n",
    "    try:\n",
    "        # Coffea-casa condor facility\n",
    "        if af == \"coffeacasa-condor\":\n",
    "            client = Client(\"tls://localhost:8786\")\n",
    "            client.register_plugin(PipInstall(packages=dependencies))\n",
    "            client.forward_logging()\n",
    "\n",
    "        # Coffea-casa dask gateway facility\n",
    "        elif af == \"coffeacasa-gateway\":\n",
    "            from dask_gateway import Gateway\n",
    "        \n",
    "            def set_env(dask_worker):\n",
    "                config_path = str(Path(dask_worker.local_directory) / 'access_token')\n",
    "                os.environ[\"BEARER_TOKEN_FILE\"] = config_path\n",
    "                os.chmod(config_path, 0o600)\n",
    "                os.chmod(\"/etc/grid-security/certificates\", 0o755)\n",
    "\n",
    "            num_workers = 150   #number of workers desired\n",
    "            gateway = Gateway()\n",
    "            clusters = gateway.list_clusters()\n",
    "            cluster = gateway.connect(clusters[0].name)\n",
    "            client = cluster.get_client()\n",
    "            cluster.scale(num_workers)\n",
    "            client.wait_for_workers(num_workers)\n",
    "            client.upload_file(\"/etc/cmsaf-secrets-chown/access_token\")\n",
    "            client.register_worker_callbacks(setup=set_env)\n",
    "            client.register_plugin(PipInstall(packages=dependencies))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(f\"The facility {af} is not implemented...\")\n",
    "            \n",
    "        print(f\"‚úÖ Connected to Dask scheduler\")\n",
    "        print(f\"üìä Dashboard: {client.dashboard_link}\")\n",
    "\n",
    "        yield client, cluster\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        print(f\"ERROR:: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        if close_after:\n",
    "            if client is not None:\n",
    "                client.close()\n",
    "                print(\"‚úÖ Client closed\")\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Setup\n",
    "\n",
    "The CMS analysis implementation is configurable via python modules, which we have to import. For this notebook, the configuration files are found in `example_cms/configs/`. You can modify the modules in this directory manually, or you can dynamically change settings using python dictionary manipulation. Below are some settings of interest that you might want to tune when you are testing your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intccms configuration import\n",
    "from example_cms.configs.configuration import config as original_config\n",
    "\n",
    "# Create a deepcopy that we can manipulate\n",
    "config = copy.deepcopy(original_config)\n",
    "\n",
    "# Limit files for testing\n",
    "config[\"datasets\"][\"max_files\"] = None # None would run over all availale files\n",
    "\n",
    "# Use local output directory\n",
    "config[\"general\"][\"output_dir\"] = \"example_cms/outputs/\"\n",
    "\n",
    "# Preprocessing (coffea) can be executed once and results loaded\n",
    "config[\"general\"][\"run_metadata_generation\"] = True # If True, run analysis pre-processing\n",
    "\n",
    "# Processer = Skimming (filter and save) + Analysis\n",
    "config[\"general\"][\"run_processor\"] = True  # If True, the coffea processor is executed\n",
    "config[\"general\"][\"run_analysis\"] = True # If True, the analysis part of the processor is executed\n",
    "config[\"general\"][\"save_skimmed_output\"] = False  # If True, skimmed events are saved to disk, otherwise filter executed on-the-fly\n",
    "\n",
    "# Analysis = Systematics + histogramming + statsitics\n",
    "config[\"general\"][\"run_histogramming\"] = True\n",
    "config[\"general\"][\"run_systematics\"] = True\n",
    "config[\"general\"][\"run_statistics\"] = False\n",
    "\n",
    "# Datasets to process, by default this is all datasets\n",
    "#config[\"general\"][\"processes\"] = [\"data\"] \n",
    "\n",
    "cli_args = [] # the code can be ran from CLI, but we don't care here\n",
    "full_config = load_config_with_restricted_cli(config, cli_args)\n",
    "\n",
    "# Validated config gives us a dictionary object with all settings checked to be safe with pydantic\n",
    "validated_config = Config(**full_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Workflow\n",
    "\n",
    "Running the CMS integration challenge workflow is split into a few steps, with a modular design that allows us flexibility. The steps are:\n",
    "\n",
    "1. Setting up output directories\n",
    "2. Building an input dataset manager\n",
    "3. Running or loading the coffea preprocessing\n",
    "4. Run the coffea processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output manager setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_manager = OutputDirectoryManager(\n",
    "    root_output_dir=validated_config.general.output_dir,\n",
    "    cache_dir=validated_config.general.cache_dir,\n",
    "    metadata_dir=validated_config.general.metadata_dir,\n",
    "    skimmed_dir=validated_config.general.skimmed_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Data Redirector (Optional)\n",
    "\n",
    "Override the redirector in the config for accessing dataset files. Useful for testing different storage backends. You can also change this in `example_cms/configs/skim.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override redirector for all datasets\n",
    "# Examples:\n",
    "#   \"root://xcache/\"                    \n",
    "#   \"root://cmsxrootd.fnal.gov/\"\n",
    "#   \"root://cms-xrd-global.cern.ch/\"\n",
    "REDIRECTOR = \"root://xcache/\"  # Change this to use a different redirector\n",
    "\n",
    "print(f\"Initial redirector  {validated_config.datasets.datasets[0].name}: {validated_config.datasets.datasets[0].redirector}\")\n",
    "\n",
    "# Apply to all datasets in config\n",
    "for dataset in validated_config.datasets.datasets:\n",
    " dataset.redirector = REDIRECTOR\n",
    "\n",
    "print(f\"Redirector set to: {REDIRECTOR}\")\n",
    "\n",
    "# Verify the change\n",
    "print(f\"New redirector:  {validated_config.datasets.datasets[0].name}: {validated_config.datasets.datasets[0].redirector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input dataset manager setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_manager = DatasetManager(validated_config.datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coffea preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_generator = DatasetMetadataManager(\n",
    "  dataset_manager=dataset_manager,\n",
    "  output_manager=output_manager,\n",
    "  config=validated_config,\n",
    ")\n",
    "\n",
    "if metadata_generator.generate_metadata:\n",
    "  with acquire_client(AF) as (client, cluster):\n",
    "      metadata_generator.run(executor=DaskExecutor(client=client))\n",
    "else:\n",
    "  metadata_generator.run()  # No client needed\n",
    "\n",
    "# Build metadata lookup and extract workitems\n",
    "metadata_lookup = metadata_generator.build_metadata_lookup()\n",
    "workitems = metadata_generator.workitems\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the coffea processor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run processor workflow\n",
    "from intccms.analysis.processor import UnifiedProcessor\n",
    "\n",
    "with acquire_client(AF) as (client, cluster):\n",
    "    # Create processor instance for MetricsCollector\n",
    "    unified_processor = UnifiedProcessor(\n",
    "        config=validated_config,\n",
    "        output_manager=output_manager,\n",
    "        metadata_lookup=metadata_lookup,\n",
    "    )\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    output, report = run_processor_workflow(\n",
    "        config=validated_config,\n",
    "        output_manager=output_manager,\n",
    "        metadata_lookup=metadata_lookup,\n",
    "        workitems=workitems,\n",
    "        executor=DaskExecutor(client=client, treereduction=8, retries=0),\n",
    "        schema=NanoAODSchema,\n",
    "    )\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Processor workflow complete in {t1-t0:.1f} seconds!\")\n",
    "\n",
    "# Print summary\n",
    "print(f\"Total events processed: {output.get('processed_events', 0):,}\")\n",
    "print(f\"Events after skim: {output.get('skimmed_events', 0):,}\")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if client is not None and not AUTO_CLOSE_CLIENT:\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
