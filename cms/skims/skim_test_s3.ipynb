{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Skimming to S3 (Parquet)\n",
    "Tests the intccms skimming infrastructure writing parquet to S3 via a coffea processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "repo_root = Path.cwd().parent\n",
    "src_dir = repo_root / \"src\"\n",
    "if str(src_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(src_dir))\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "try:\n",
    "    import omegaconf\n",
    "except Exception:\n",
    "    ! pip install omegaconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from intccms.utils.tools import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "INPUT_FILE = \"root://xcache//store/mc/RunIISummer20UL18NanoAODv9/TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8/NANOAODSIM/106X_upgrade2018_realistic_v16_L1v1-v1/120000/0E9EA19A-AE0E-3149-88C3-D733240FF5AB.root\"\n",
    "DATASET_NAME = \"ttbar\"\n",
    "TREE_NAME = \"Events\"\n",
    "CHUNK_SIZE = 100_000\n",
    "\n",
    "# S3 configuration\n",
    "AWS_ID = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "AWS_SECRET = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "print(AWS_ID, AWS_SECRET)\n",
    "S3_ENDPOINT = \"https://red-s3.unl.edu/cmsaf-test-oshadura\"\n",
    "OUTPUT_DIR = \"s3:///skim_test_s3_out\"\n",
    "\n",
    "STORAGE_OPTIONS = {\n",
    "    \"key\": AWS_ID,\n",
    "    \"secret\": AWS_SECRET,\n",
    "    \"client_kwargs\": {\"endpoint_url\": S3_ENDPOINT},\n",
    "}\n",
    "\n",
    "# AF options: [iterative, coffeacasa-condor, coffeacasa-gateway, purdue-af]\n",
    "AF = \"coffeacasa-condor\"\n",
    "NUM_WORKERS = 10\n",
    "AUTO_CLOSE_CLIENT = False\n",
    "\n",
    "BRANCHES = {\n",
    "    \"event\": [\"run\", \"luminosityBlock\", \"event\"],\n",
    "    \"Muon\": [\"pt\", \"eta\", \"phi\", \"mass\"],\n",
    "    \"PuppiMET\": [\"pt\", \"phi\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import time\n",
    "\n",
    "import awkward as ak\n",
    "import cloudpickle\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "from coffea.nanoevents import NanoAODSchema\n",
    "from coffea.processor import ProcessorABC, Runner, IterativeExecutor, DaskExecutor\n",
    "\n",
    "import intccms\n",
    "from intccms.schema import SkimmingConfig\n",
    "from intccms.skimming.io.writers import ParquetWriter\n",
    "from intccms.skimming.pipeline.stages import build_column_list, extract_columns, save_events\n",
    "from intccms.utils.dask_client import acquire_client, live_prints\n",
    "from intccms.utils.functors import SelectionExecutor\n",
    "\n",
    "cloudpickle.register_pickle_by_value(intccms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skim_selection(puppimet, hlt):\n",
    "    selection = PackedSelection()\n",
    "    selection.add(\"trigger\", hlt.Mu50)\n",
    "    selection.add(\"met_cut\", puppimet.pt > 50)\n",
    "    selection.add(\"skim\", selection.all(\"trigger\", \"met_cut\"))\n",
    "    return selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "processor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkimProcessor(ProcessorABC):\n",
    "\n",
    "    def __init__(self, skim_config, branches, output_dir, storage_options=None):\n",
    "        self.skim_config = skim_config\n",
    "        self.output_dir = output_dir\n",
    "        self.storage_options = storage_options or {}\n",
    "        self.columns_to_keep, _ = build_column_list(branches)\n",
    "        self.writer = ParquetWriter()\n",
    "\n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return {\"total_in\": 0, \"total_out\": 0}\n",
    "\n",
    "    def _build_output_path(self, events):\n",
    "        file_hash = hashlib.md5(events.metadata[\"filename\"].encode()).hexdigest()[:8]\n",
    "        entry_start = events.metadata[\"entrystart\"]\n",
    "        entry_stop = events.metadata[\"entrystop\"]\n",
    "        dataset = events.metadata[\"dataset\"]\n",
    "        return f\"{self.output_dir.rstrip('/')}/{dataset}/{file_hash}_{entry_start}_{entry_stop}.parquet\"\n",
    "\n",
    "    def process(self, events):\n",
    "        output = self.accumulator\n",
    "        dataset = events.metadata[\"dataset\"]\n",
    "        n_in = len(events)\n",
    "\n",
    "        executor = SelectionExecutor(self.skim_config)\n",
    "        mask = executor.execute(events)\n",
    "        events = events[mask]\n",
    "        n_out = len(events)\n",
    "\n",
    "        if n_out > 0:\n",
    "            output_columns = extract_columns(events, self.columns_to_keep)\n",
    "            out_path = self._build_output_path(events)\n",
    "            save_events(\n",
    "                self.writer, output_columns, out_path,\n",
    "                compression=\"zstd\", storage_options=self.storage_options,\n",
    "            )\n",
    "\n",
    "        output[\"total_in\"] = n_in\n",
    "        output[\"total_out\"] = n_out\n",
    "        print(f\"  {dataset} [{events.metadata['entrystart']}:{events.metadata['entrystop']}]: {n_out}/{n_in} events passed\")\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "run",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f130970124b489bb50fdf451bb6c9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707642beebee432ca86edfbed219194d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done in 43.7s: 95,634/1,290,000 events kept (7.4%)\n"
     ]
    }
   ],
   "source": [
    "fileset = {DATASET_NAME: {\"files\": [INPUT_FILE], \"treename\": TREE_NAME}}\n",
    "\n",
    "skim_config = SkimmingConfig(\n",
    "    function=skim_selection,\n",
    "    use=[(\"PuppiMET\", None), (\"HLT\", None)],\n",
    ")\n",
    "\n",
    "processor = SkimProcessor(\n",
    "    skim_config=skim_config,\n",
    "    branches=BRANCHES,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    storage_options=STORAGE_OPTIONS,\n",
    ")\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "if AF == \"iterative\":\n",
    "    runner = Runner(\n",
    "        executor=IterativeExecutor(),\n",
    "        schema=NanoAODSchema,\n",
    "        chunksize=CHUNK_SIZE,\n",
    "    )\n",
    "    output = runner(fileset, treename=TREE_NAME, processor_instance=processor)\n",
    "else:\n",
    "    with acquire_client(AF, num_workers=NUM_WORKERS, close_after=AUTO_CLOSE_CLIENT, propagate_aws_env=True) as (client, cluster):\n",
    "        #stop = live_prints(client)\n",
    "        runner = Runner(\n",
    "            executor=DaskExecutor(client=client),\n",
    "            schema=NanoAODSchema,\n",
    "            chunksize=CHUNK_SIZE,\n",
    "        )\n",
    "        output = runner(fileset, treename=TREE_NAME, processor_instance=processor)\n",
    "        #stop.set()\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "total_in = output[\"total_in\"]\n",
    "total_out = output[\"total_out\"]\n",
    "print(f\"\\nDone in {t1-t0:.1f}s: {total_out:,}/{total_in:,} events kept ({100*total_out/total_in:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dhz7g8risg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3:///skim_test_s3_out/ttbar/39ccc721_0_99231.parquet\n",
      "Read back 39ccc721_0_99231.parquet: 7492 events, fields=['run', 'luminosityBlock', 'event', 'Muon', 'PuppiMET']\n"
     ]
    }
   ],
   "source": [
    "from intccms.skimming.io.readers import get_reader\n",
    "\n",
    "reader = get_reader(\"parquet\")\n",
    "file_hash = hashlib.md5(INPUT_FILE.encode()).hexdigest()[:8]\n",
    "test_file = f\"{OUTPUT_DIR.rstrip('/')}/{DATASET_NAME}/{file_hash}_0_99231.parquet\"\n",
    "print(test_file)\n",
    "events = reader.read(test_file, tree_name=None, storage_options=STORAGE_OPTIONS)\n",
    "print(f\"Read back {test_file.split('/')[-1]}: {len(events)} events, fields={events.fields}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "if AF != \"iterative\" and not AUTO_CLOSE_CLIENT:\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3085eef5-fd00-4837-bbe1-a7575ea1ef30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
