{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data Inspector Demo\n",
    "\n",
    "This notebook demonstrates the `intccms.metrics.inspector` module for characterizing input ROOT files.\n",
    "\n",
    "The inspector allows you to:\n",
    "- Extract metadata from ROOT files (events, file sizes, branch sizes, compression ratios)\n",
    "- Run distributed inspection using Dask\n",
    "- Aggregate statistics across datasets\n",
    "- Create visualizations of input data characteristics\n",
    "\n",
    "**Works directly with DatasetManager**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to Python path\n",
    "repo_root = Path.cwd()\n",
    "src_dir = repo_root / \"src\"\n",
    "examples_dir = repo_root/\"example_cms\"\n",
    "if str(src_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(src_dir))\n",
    "if str(examples_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(examples_dir))\n",
    "print(f\"✅ Added {src_dir} to Python path\")\n",
    "print(f\"✅ Added {examples_dir} to Python path\")\n",
    "\n",
    "\n",
    "from dask.distributed import Client, PipInstall\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cloudpickle\n",
    "import intccms\n",
    "import example_cms\n",
    "\n",
    "# Register modules for cloud pickle\n",
    "cloudpickle.register_pickle_by_value(intccms)\n",
    "cloudpickle.register_pickle_by_value(example_cms)\n",
    "\n",
    "from intccms.datasets import DatasetManager\n",
    "from intccms.metrics.inspector import (\n",
    "    extract_files_from_dataset_manager,\n",
    "    get_dataset_file_counts,\n",
    "    inspect_dataset_distributed,\n",
    "    aggregate_statistics,\n",
    "    group_by_dataset,\n",
    "    compute_dataset_statistics,\n",
    "    compute_compression_stats,\n",
    "    format_error_summary,\n",
    "    plot,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_client():\n",
    "    client = Client(\"tls://localhost:8786\")\n",
    "    cluster = None  # no local cluster in this mode\n",
    "    return client, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset Configuration\n",
    "\n",
    "Load the dataset configuration from `example_cms/configs/skim.py`.\n",
    "This uses the same configuration as your processing workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from example_cms.configs.configuration import config as original_config\n",
    "from intccms.schema import Config, load_config_with_restricted_cli\n",
    "# Configuration setup\n",
    "config = copy.deepcopy(original_config)\n",
    "\n",
    "cli_args = []\n",
    "full_config = load_config_with_restricted_cli(config, cli_args)\n",
    "validated_config = Config(**full_config)\n",
    "\n",
    "# Create DatasetManager\n",
    "dm = DatasetManager(validated_config.datasets)\n",
    "\n",
    "print(\"Available datasets:\")\n",
    "for name in dm.datasets.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Quick File Count Summary\n",
    "\n",
    "Get a quick count of files per dataset without full inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_counts = get_dataset_file_counts(dm)\n",
    "\n",
    "print(\"\\nFile counts per dataset:\")\n",
    "for dataset, count in file_counts.items():\n",
    "    print(f\"  {dataset}: {count} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract Files for Inspection\n",
    "\n",
    "Extract file paths from DatasetManager. You can:\n",
    "- Inspect all datasets\n",
    "- Inspect specific processes\n",
    "- Limit files per process (useful for quick sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Sample first 5 files per dataset for quick testing\n",
    "# file_list, dataset_map = extract_files_from_dataset_manager(\n",
    "#     dm,\n",
    "#     max_files_per_process=5,\n",
    "# )\n",
    "\n",
    "# Option B: Inspect specific datasets\n",
    "# file_list, dataset_map = extract_files_from_dataset_manager(\n",
    "#     dm,\n",
    "#     processes=[\"signal\", \"ttbar_semilep\"],\n",
    "#     max_files_per_process=10,\n",
    "# )\n",
    "\n",
    "# Option C: Inspect all files (can be slow for large datasets!)\n",
    "file_list, dataset_map = extract_files_from_dataset_manager(dm)\n",
    "\n",
    "print(f\"\\nExtracted {len(file_list)} files for inspection\")\n",
    "print(f\"Example file: {file_list[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Distributed Inspection with Dask\n",
    "\n",
    "Run distributed file inspection using Dask.\n",
    "This extracts metadata from all files in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Start a local Dask cluster\n",
    "    client, _ = acquire_client()\n",
    "    print(f\"Dask dashboard: {client.dashboard_link}\")\n",
    "    # Run distributed inspection\n",
    "    # Note: max_branches limits the number of branches inspected per file for faster results\n",
    "    results, errors = inspect_dataset_distributed(\n",
    "        client,\n",
    "        file_list,\n",
    "        max_branches=500,  # Limit to first 100 branches for speed\n",
    "    )\n",
    "    \n",
    "    # Print formatted error summary\n",
    "    print(format_error_summary(errors))\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\n=== Example Result ===\")\n",
    "        print(f\"  File: {results[0]['filepath']}\")\n",
    "        print(f\"  Events: {results[0]['num_events']:,}\")\n",
    "        print(f\"  Branches: {results[0]['num_branches']}\")\n",
    "    else:\n",
    "        print(\"\\nNo files were successfully inspected!\")\n",
    "        \n",
    "finally:\n",
    "    client.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Aggregate Statistics\n",
    "\n",
    "Compute aggregate statistics across all inspected files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Fetch File Sizes from Rucio\n",
    "\n",
    "Use the inspector's Rucio helper to retrieve authoritative file sizes. This\n",
    "requires a valid Rucio environment (credentials and network access). If the\n",
    "lookup fails, the notebook continues with locally derived statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intccms.metrics.inspector import rucio as inspector_rucio\n",
    "from rich.console import Console\n",
    "\n",
    "size_summary = None\n",
    "try:\n",
    "    size_summary = inspector_rucio.fetch_file_sizes(\n",
    "        dm,\n",
    "        processes=[\"signal\", \"ttbar_semilep\"],\n",
    "        max_files_per_process=5,\n",
    "    )\n",
    "    console = Console(force_jupyter=False)\n",
    "    console.print(inspector_rucio.format_dataset_size_table(size_summary))\n",
    "except Exception as exc:\n",
    "    print(\"Skipping Rucio size lookup (set size_summary=None):\", exc)\n",
    "    size_summary = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from intccms.metrics.inspector import (\n",
    "    format_overall_stats_table,\n",
    "    format_branch_stats_table,\n",
    "    format_dataset_stats_table,\n",
    "    format_compression_stats_table,\n",
    ")\n",
    "\n",
    "# Create console\n",
    "console = Console(force_jupyter=False)\n",
    "\n",
    "# Aggregate and display statistics\n",
    "stats = aggregate_statistics(results, size_summary=None)\n",
    "table = format_overall_stats_table(stats)\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Branch statistics\n",
    "\n",
    "Analyze branch size and compression distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intccms.metrics.inspector.aggregator import compute_branch_statistics\n",
    "\n",
    "branch_stats = compute_branch_statistics(results)\n",
    "\n",
    "table = format_branch_stats_table(branch_stats)\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Per-Dataset Statistics\n",
    "\n",
    "Group results by dataset and compute per-dataset statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by dataset\n",
    "grouped = group_by_dataset(results, dataset_map)\n",
    "\n",
    "# Compute per-dataset statistics\n",
    "dataset_stats = compute_dataset_statistics(grouped)\n",
    "\n",
    "# Display as rich table\n",
    "table = format_dataset_stats_table(dataset_stats)\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Compression Statistics\n",
    "\n",
    "Analyze compression ratios across all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_stats = compute_compression_stats(results)\n",
    "# Display as rich table\n",
    "table = format_compression_stats_table(compression_stats)\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Visualizations\n",
    "\n",
    "Create plots to visualize the inspection results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Events per file ratio by dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Events per file ratio by dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot.plot_events_per_file_by_dataset(dataset_stats)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot.plot_event_distribution(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plot.plot_dataset_comparison(dataset_stats)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Box Plots\n",
    "\n",
    "The box plots in this analysis show statistical distributions:\n",
    "\n",
    "- **Box**: Contains the middle 50% of data (interquartile range, IQR)\n",
    "- **Line inside box**: Median value (50th percentile)\n",
    "- **Whiskers**: Extend to the 5th and 95th percentiles\n",
    "- **Points beyond whiskers**: Outliers outside the 5th-95th percentile range\n",
    "\n",
    "This visualization helps identify data skewness, outliers, and distribution characteristics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branch Size Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot.plot_branch_size_distribution(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branch Compression Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot.plot_branch_compression_distribution(results)\n",
    "if fig is not None:\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No compression data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branch Distributions by Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plot.plot_branch_distributions_by_dataset(results, dataset_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Size Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot.plot_file_size_distribution(results)\n",
    "if fig is not None:\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No file size data available (all files are remote)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Dashboard\n",
    "\n",
    "Create a comprehensive dashboard with all key plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot.plot_summary_dashboard(results, dataset_stats, dataset_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save Results\n",
    "\n",
    "You can save plots to files and export statistics to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save summary dashboard\n",
    "# fig = plot.plot_summary_dashboard(\n",
    "#     results, dataset_stats, top_branches,\n",
    "#     save_path=\"input_summary.png\"\n",
    "# )\n",
    "\n",
    "# Export statistics to JSON\n",
    "# output = {\n",
    "#     \"overall_stats\": stats,\n",
    "#     \"dataset_stats\": dataset_stats,\n",
    "#     \"compression_stats\": compression_stats,\n",
    "#     \"top_branches\": [\n",
    "#         {\"name\": name, \"size_bytes\": size, \"compression_ratio\": ratio}\n",
    "#         for name, size, ratio in top_branches\n",
    "#     ],\n",
    "# }\n",
    "# \n",
    "# with open(\"inspection_results.json\", \"w\") as f:\n",
    "#     json.dump(output, f, indent=2)\n",
    "\n",
    "print(\"Done! You can save plots and export statistics as needed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
